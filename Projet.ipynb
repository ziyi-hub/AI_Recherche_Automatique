{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Initialization #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directly from data ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "它将Python列表data转换为张量x_data<br/>这段代码的功能是创建了一个2x2的张量x_data，其中包含了[[1, 2], [3, 4]]这个数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From a NumPy array ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这段代码的功能是将Python列表data转换为NumPy数组np_array，然后将NumPy数组转换为PyTorch张量x_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From another tensor ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_ones = torch.ones_like(x_data)：这行代码创建了一个与x_data张量具有相同形状的张量x_ones，其中所有元素的值都设为1。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_rand = torch.rand_like(x_data, dtype=torch.float)：这行代码创建了一个与x_data张量具有相同形状的张量x_rand，但其中的值是在0到1之间的随机数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.6698, 0.4881],\n",
      "        [0.5534, 0.6052]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With random or constant values ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shape = (2, 3,)：这行代码定义了一个元组shape，其中包含了张量的形状信息。在这个例子中，形状为2行3列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.4076, 0.5438, 0.4143],\n",
      "        [0.2474, 0.8300, 0.6718]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "shape = (2, 3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Attributes #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这行代码创建了一个形状为(3, 4)的随机张量tensor。torch.rand()函数用于创建指定形状的张量，并用0到1之间的随机数填充。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3, 4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Operations #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这行代码将张量tensor移动到GPU上进行计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device tensor is stored on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# We move our tensor to the GPU if available\n",
    "if torch.cuda.is_available():\n",
    "  tensor = tensor.to('cuda')\n",
    "  print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard numpy-like indexing and slicing ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这段代码使用了PyTorch库创建了一个4x4的张量（tensor），并将其所有元素初始化为1。然后，通过索引操作，将张量的第二列（索引为1的列，Python中索引从0开始）的所有元素设置为0。最后，打印输出了修改后的张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这段代码使用了PyTorch库中的torch.cat()函数，该函数用于沿指定维度拼接张量。给定的张量列表是[tensor, tensor, tensor]，即将tensor张量在水平方向（dim=1，即列的方向）上连接三次。假设初始的tensor是一个4x4的张量，那么沿着列的方向连接三次就会得到一个4x12的张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim=0)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiplying tensors ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "两种写法都会得到一个新的张量，其中每个元素是对应位置上两个原始张量对应位置上元素的乘积。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.mul(tensor) \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor * tensor \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# This computes the element-wise product\n",
    "print(f\"tensor.mul(tensor) \\n {tensor.mul(tensor)} \\n\")\n",
    "# Alternative syntax:\n",
    "print(f\"tensor * tensor \\n {tensor * tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这段代码使用了PyTorch库中的矩阵乘法操作，计算了一个张量与其转置矩阵的矩阵乘积。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.matmul(tensor.T) \n",
      " tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]]) \n",
      "\n",
      "tensor @ tensor.T \n",
      " tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"tensor.matmul(tensor.T) \\n {tensor.matmul(tensor.T)} \\n\")\n",
    "# Alternative syntax:\n",
    "print(f\"tensor @ tensor.T \\n {tensor @ tensor.T}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor([[6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor, \"\\n\")\n",
    "tensor.add_(5) #函数名以_结尾的操作通常表示这是一个原地操作，也就是说，该操作会直接修改原始张量的数值而不返回新的张量。在这里，add_()函数执行了原地加法操作。\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bridge with NumPy #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors on the CPU and NumPy arrays can share their underlying memory locations, and changing one will change the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor to NumPy array #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.])\n",
      "n: [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.])\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "t.add_(1)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NumPy array to Tensor #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "np.add(n, 1, out=n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# A Gentle Introduction to torch.autograd #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/lib/oar/.batch_job_bashrc: line 5: /home/ziwang/.bashrc: No such file or directory\n",
      "Requirement already satisfied: torchvision in ./.local/lib/python3.9/site-packages (0.17.0)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from torchvision) (2.25.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/lib/python3/dist-packages (from torchvision) (8.1.2)\n",
      "Requirement already satisfied: torch==2.2.0 in ./.local/lib/python3.9/site-packages (from torchvision) (2.2.0)\n",
      "Requirement already satisfied: numpy in /usr/lib/python3/dist-packages (from torchvision) (1.19.5)\n",
      "Requirement already satisfied: sympy in ./.local/lib/python3.9/site-packages (from torch==2.2.0->torchvision) (1.12)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch==2.2.0->torchvision) (2.11.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.local/lib/python3.9/site-packages (from torch==2.2.0->torchvision) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.local/lib/python3.9/site-packages (from torch==2.2.0->torchvision) (8.9.2.26)\n",
      "Requirement already satisfied: triton==2.2.0 in ./.local/lib/python3.9/site-packages (from torch==2.2.0->torchvision) (2.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.local/lib/python3.9/site-packages (from torch==2.2.0->torchvision) (4.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.local/lib/python3.9/site-packages (from torch==2.2.0->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.local/lib/python3.9/site-packages (from torch==2.2.0->torchvision) (12.1.3.1)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from torch==2.2.0->torchvision) (3.0.12)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.local/lib/python3.9/site-packages (from torch==2.2.0->torchvision) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.local/lib/python3.9/site-packages (from torch==2.2.0->torchvision) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.local/lib/python3.9/site-packages (from torch==2.2.0->torchvision) (12.1.105)\n",
      "Requirement already satisfied: fsspec in ./.local/lib/python3.9/site-packages (from torch==2.2.0->torchvision) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in ./.local/lib/python3.9/site-packages (from torch==2.2.0->torchvision) (2.19.3)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.local/lib/python3.9/site-packages (from torch==2.2.0->torchvision) (12.1.0.106)\n",
      "Requirement already satisfied: networkx in /usr/lib/python3/dist-packages (from torch==2.2.0->torchvision) (2.5)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.local/lib/python3.9/site-packages (from torch==2.2.0->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.local/lib/python3.9/site-packages (from torch==2.2.0->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.local/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0->torchvision) (12.3.101)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.local/lib/python3.9/site-packages (from sympy->torch==2.2.0->torchvision) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "# 创建了一个ResNet-18模型的实例，并指定了使用默认的预训练权重。\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "# data = torch.rand(1, 3, 64, 64): 创建了一个形状为(1, 3, 64, 64)的随机张量，表示输入数据。\n",
    "# 这个张量的形状是(batch_size, channels, height, width)，对应于一批大小为1的RGB图像，每个图像的尺寸为64x64。\n",
    "data = torch.rand(1, 3, 64, 64) \n",
    "\n",
    "# 创建了一个形状为(1, 1000)的随机张量，表示模型的输出标签。这个张量的形状是(batch_size, num_classes)，对应于模型输出的概率分布，其中包含了1000个类别的预测概率。\n",
    "labels = torch.rand(1, 1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "接下来，我们将输入数据通过模型的每一层进行预测。这就是前向传递。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction = model(data) # forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "我们使用模型的预测结果和相应的标签来计算误差（损失）。下一步是通过网络反向传播误差。当我们在误差张量上调用 .backward() 时，反向传播就开始了。\n",
    "然后，Autograd 会计算每个模型参数的梯度，并存储在参数的 .grad 属性中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss = (prediction - labels).sum()\n",
    "loss.backward() # backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "接下来，我们加载一个优化器，在本例中，SGD 的学习率为 0.01，动量为 0.9。我们在优化器中注册模型的所有参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "最后，我们调用 .step() 启动梯度下降。优化器会根据 .grad 中存储的梯度调整每个参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "optim.step() #gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Neural Networks #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "A typical training procedure for a neural network is as follows:\n",
    "\n",
    "Define the neural network that has some learnable parameters (or weights)\n",
    "\n",
    "Iterate over a dataset of inputs\n",
    "\n",
    "Process input through the network\n",
    "\n",
    "Compute the loss (how far is the output from being correct)\n",
    "\n",
    "Propagate gradients back into the network’s parameters\n",
    "\n",
    "Update the weights of the network, typically using a simple update rule: weight = weight - learning_rate * gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "神经网络的典型训练过程如下：\n",
    "\n",
    "定义具有某些可学习参数（或权重）的神经网络\n",
    "\n",
    "对输入数据集进行迭代\n",
    "\n",
    "通过网络处理输入\n",
    "\n",
    "计算损失（输出离正确还有多远）\n",
    "\n",
    "将梯度传回网络参数中\n",
    "\n",
    "更新网络权重，通常使用简单的更新规则：权重 = 权重 - 学习率 * 梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Define the network ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 定义了一个名为Net的类，它是nn.Module类的子类，表示这是一个神经网络模型。\n",
    "class Net(nn.Module):\n",
    "\n",
    "    # 这是Net类的构造函数，用于初始化模型的各个层。\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # 第一个参数是输入通道数，第二个参数是输出通道数，第三个参数是卷积核的大小。\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        # 这三行代码定义了三个全连接层。nn.Linear表示线性变换，第一个参数是输入特征数，第二个参数是输出特征数。\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    # 前向传播函数\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window 这两行代码进行了卷积层后的最大池化操作，用于提取特征并降低维度。\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square, you can specify with a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension 将输入张量展平，保留批次维度。\n",
    "        \n",
    "        #这三行代码应用了ReLU激活函数和线性变换来处理全连接层的输出\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1的权重是一个四维张量，形状为[6, 1, 5, 5]。这表示有6个输出通道、1个输入通道，每个通道使用大小为5x5的卷积核。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0661, -0.0597, -0.0166, -0.0203, -0.1521, -0.1785,  0.1216,  0.0531,\n",
      "          0.0594, -0.0248]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32) #这表示一个包含一个样本的批次，每个样本有一个通道，并且是 32x32 的图像\n",
    "out = net(input)\n",
    "print(out) #输出结果 out，它是一个大小为 (1, 10) 的张量。每个元素表示对应类别的预测分数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "net.zero_grad() #zero_grad() 来清除之前的梯度，以避免梯度的累积影响下一次的计算。\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "torch.nn 仅支持迷你批次。整个 torch.nn 软件包只支持迷你样本批次的输入，而不支持单个样本的输入。\n",
    "\n",
    "例如，nn.Conv2d 将接收 nSamples x nChannels x Height x Width 的 4D 张量。\n",
    "\n",
    "如果只有单个样本，只需使用 input.unsqueeze(0) 添加一个假的批次维度即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Loss Function ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "损失函数接收（输出、目标）这对输入，并计算出一个值来估计输出与目标之间的距离。\n",
    "\n",
    "nn 软件包中有几种不同的损失函数。一个简单的损失函数是：nn.MSELoss，它计算输出与目标之间的均方误差。\n",
    "\n",
    "例如"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9036, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)  # a dummy target, for example\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "criterion = nn.MSELoss() # 表示均方误差损\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward0 object at 0x7fc1b248a970>\n",
      "<AddmmBackward0 object at 0x7fc1b248adc0>\n",
      "<AccumulateGrad object at 0x7fc1b248a970>\n"
     ]
    }
   ],
   "source": [
    "# MSELoss 这行代码打印了损失函数的反向传播函数，即均方误差损失函数 MSELoss。\n",
    "print(loss.grad_fn)  \n",
    "\n",
    "# 这行代码通过 next_functions 访问了损失函数计算过程中的下一级函数。由于均方误差损失函数是通过 output 和 target 计算得到的，\n",
    "# 因此 next_functions 中的第一个元素对应于 output，即模型的输出。因为我们的模型最后一层是全连接层 Linear，所以这里打印出来的是 Linear 函数。\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear \n",
    "\n",
    "# 这行代码进一步访问了损失函数计算过程中下一级的函数。在这里，我们进入了全连接层 Linear 的计算过程。\n",
    "# 在全连接层中，输入经过线性变换后会被传递给激活函数，而在我们的模型中，激活函数是 ReLU。所以这里打印出来的是 ReLU 函数。\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## BackProp ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "要反向传播错误，我们只需 loss.backward()。不过您需要清除现有的梯度，否则梯度将累积到现有梯度中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "None\n",
      "conv1.bias.grad after backward\n",
      "tensor([-0.0036,  0.0178,  0.0011,  0.0194, -0.0087,  0.0021])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Update the weights ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "实际应用中最简单的更新规则是随机梯度下降法（SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "weight = weight - learning_rate * gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 梯度下降的迭代过程，从而使模型的参数朝着损失函数减小的方向更新。\n",
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "不过，由于使用的是神经网络，因此需要使用各种不同的更新规则，如 SGD、Nesterov-SGD、Adam、RMSProp 等。为此，我们开发了一个小软件包：torch.optim，用于实现所有这些方法。使用它非常简单："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Training a Classifier #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Training an image classifier ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "我们将依次执行以下步骤:\n",
    "\n",
    "- 使用 torchvision 加载 CIFAR10 训练数据集和测试数据集并将其标准化\n",
    "- 定义卷积神经网络\n",
    "- 定义损失函数\n",
    "- 在训练数据上训练网络\n",
    "- 在测试数据上测试网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 1. Load and normalize CIFAR10 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "torchvision 数据集的输出是范围为 [0, 1] 的 PILImage 图像。我们将其转换为归一化范围为 [-1, 1] 的张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose( # 创建了一个转换组合，将一系列的数据预处理操作组合在一起。这里使用了两个预处理操作\n",
    "    [transforms.ToTensor(), # 将图像转换为 PyTorch 张量，并将像素值缩放到 [0, 1] 的范围\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) # 对图像进行标准化处理，减去均值（0.5）并除以标准差（0.5）\n",
    "\n",
    "batch_size = 4 #指定了数据加载器每次加载的批次大小，这里设置为 4。\n",
    "\n",
    "# 创建了 CIFAR-10 数据集的训练集对象。root 参数指定了数据集存储的根目录，train=True 表示加载训练集，\n",
    "# download=True 表示如果数据集不存在则自动下载，transform=transform 表示应用之前定义的数据预处理操作。\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "# 创建了一个训练集数据加载器。trainloader 负责从训练集中加载数据，shuffle=True 表示每个 epoch 都会对数据进行洗牌，\n",
    "# num_workers=2 表示使用两个子进程来加载数据以加快速度。\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "# 这里分别创建了 CIFAR-10 数据集的测试集对象和测试集数据加载器。\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deer  cat   frog  deer \n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "# 在显示之前，需要进行反标准化操作，即将图像的像素值从 [-1, 1] 的范围还原到 [0, 1] 的范围，\n",
    "# 然后将张量转换为 NumPy 数组，并进行维度转置\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0))) \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader) #创建了一个迭代器 dataiter，用于从训练集数据加载器中迭代获取数据。\n",
    "images, labels = next(dataiter) #从迭代器中获取了一批数据，其中 images 是图像数据，labels 是对应的标签。\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images)) #用于将一批图像组合成一个网格，方便显示\n",
    "# print labels\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 2. Define a Convolutional Neural Network ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 3.Define a Loss function and optimizer ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "#这行代码定义了损失函数，即交叉熵损失函数。交叉熵损失函数通常用于多类别分类问题\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#这行代码定义了优化器，即随机梯度下降（SGD）优化器。SGD是一种常用的优化算法，用于更新神经网络的权重以最小化损失函数。\n",
    "#lr=0.001指定了学习率，即每次更新时的步长。momentum=0.9是SGD的一个超参数，用于加速SGD在相关方向上前进，并减小波动。\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 4.Train the network ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (4x150 and 400x120)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[81], line 20\u001B[0m\n\u001B[1;32m     16\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m#损失函数criterion计算模型预测值outputs与真实标签labels之间的损失。\u001B[39;00m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# forward + backward + optimize\u001B[39;00m\n\u001B[0;32m---> 20\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mnet\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     21\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels)\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m#调用backward()方法进行反向传播，计算损失对模型参数的梯度。\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[71], line 20\u001B[0m, in \u001B[0;36mNet.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     18\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpool(F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv2(x)))\n\u001B[1;32m     19\u001B[0m x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mflatten(x, \u001B[38;5;241m1\u001B[39m) \u001B[38;5;66;03m# flatten all dimensions except batch\u001B[39;00m\n\u001B[0;32m---> 20\u001B[0m x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfc1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     21\u001B[0m x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc2(x))\n\u001B[1;32m     22\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc3(x)\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/linear.py:116\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    115\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 116\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (4x150 and 400x120)"
     ]
    }
   ],
   "source": [
    "# for epoch in range(2):：这是一个外部循环，用于迭代整个数据集多次，每次称为一个\"epoch\"。\n",
    "# 在这个示例中，数据集将被遍历两次。\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    # 初始化一个变量running_loss，用于跟踪每个epoch中的累积损失。\n",
    "    running_loss = 0.0\n",
    "\n",
    "    #这是一个内部循环，用于遍历训练数据加载器trainloader中的每个批次\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        # 解压批次数据，将输入和对应的标签分别赋给inputs和labels变量。\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        # 将输入数据inputs通过神经网络net进行前向传播，得到输出。\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #损失函数criterion计算模型预测值outputs与真实标签labels之间的损失。\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        #调用backward()方法进行反向传播，计算损失对模型参数的梯度。\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        #累积当前批次的损失到running_loss变量中。\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        #每遍历2000个批次（mini-batches），打印一次损失。这个条件用于控制打印频率。\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "\n",
    "            #打印当前epoch和批次数，以及当前2000个mini-batches的平均损失。\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "\n",
    "            #重置running_loss，以便计算下一个2000个mini-batches的损失。\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATH = './cifar_net.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 5. Test the network on the test data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroundTruth:  cat   ship  ship  plane\n"
     ]
    }
   ],
   "source": [
    "# 这一行创建了一个迭代器（iterator），通过 testloader 对象来迭代测试数据集。\n",
    "# 使用批处理（batch processing）的方式来处理这些数据，每次处理4张图像。\n",
    "dataiter = iter(testloader) \n",
    "\n",
    "# next(dataiter) 来获取下一个批次的数据\n",
    "# images 是一个张量（tensor），其中包含了2张图像的数据\n",
    "# labels 是一个张量，其中包含了这2张图像对应的标签\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，让我们重新加载已保存的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Net:\n\tsize mismatch for conv2.weight: copying a param with shape torch.Size([16, 6, 5, 5]) from checkpoint, the shape in current model is torch.Size([6, 6, 5, 5]).\n\tsize mismatch for conv2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([6]).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[78], line 5\u001B[0m\n\u001B[1;32m      1\u001B[0m net \u001B[38;5;241m=\u001B[39m Net()\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# 这一行从指定路径 PATH 加载了一个保存的模型参数\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# torch.load 函数用于加载 PyTorch 模型的参数，PATH 是保存模型参数的文件路径\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# net.load_state_dict(...)：这一行将加载的模型参数应用到新创建的神经网络模型 net 上\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m \u001B[43mnet\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_state_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mPATH\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:2153\u001B[0m, in \u001B[0;36mModule.load_state_dict\u001B[0;34m(self, state_dict, strict, assign)\u001B[0m\n\u001B[1;32m   2148\u001B[0m         error_msgs\u001B[38;5;241m.\u001B[39minsert(\n\u001B[1;32m   2149\u001B[0m             \u001B[38;5;241m0\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMissing key(s) in state_dict: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   2150\u001B[0m                 \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mk\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m missing_keys)))\n\u001B[1;32m   2152\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(error_msgs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m-> 2153\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mError(s) in loading state_dict for \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   2154\u001B[0m                        \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(error_msgs)))\n\u001B[1;32m   2155\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Error(s) in loading state_dict for Net:\n\tsize mismatch for conv2.weight: copying a param with shape torch.Size([16, 6, 5, 5]) from checkpoint, the shape in current model is torch.Size([6, 6, 5, 5]).\n\tsize mismatch for conv2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([6])."
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "# 这一行从指定路径 PATH 加载了一个保存的模型参数\n",
    "# torch.load 函数用于加载 PyTorch 模型的参数，PATH 是保存模型参数的文件路径\n",
    "# net.load_state_dict(...)：这一行将加载的模型参数应用到新创建的神经网络模型 net 上\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (4x150 and 400x120)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[79], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mnet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[71], line 20\u001B[0m, in \u001B[0;36mNet.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     18\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpool(F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv2(x)))\n\u001B[1;32m     19\u001B[0m x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mflatten(x, \u001B[38;5;241m1\u001B[39m) \u001B[38;5;66;03m# flatten all dimensions except batch\u001B[39;00m\n\u001B[0;32m---> 20\u001B[0m x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfc1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     21\u001B[0m x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc2(x))\n\u001B[1;32m     22\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc3(x)\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/linear.py:116\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    115\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 116\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (4x150 and 400x120)"
     ]
    }
   ],
   "source": [
    "outputs = net(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.9983, -3.5876,  0.9514,  3.0810,  1.5838,  2.6677,  2.3314,  0.9051,\n",
      "         -2.1373, -3.0755],\n",
      "        [-1.0189, -3.3774,  0.8732,  3.1156,  0.8819,  4.0162, -0.5210,  2.3662,\n",
      "         -2.4940, -1.9746],\n",
      "        [-0.1186,  0.1551,  0.7756, -0.2569,  2.8411, -0.3480, -0.1882,  0.1662,\n",
      "         -2.3091, -1.2246],\n",
      "        [-0.7391, -2.0855,  0.1290, -0.1564,  3.4124,  0.2618, -1.5349,  6.3392,\n",
      "         -3.4793, -1.3070]])\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们观察outputs有4个样本（4行），每个样本有10个类别的预测结果（10列）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出是 10 个类别的能量。某个类别的能量越高，网络就越认为图像属于该类别。因此，我们来获取最高能量的指数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  cat   ship  ship  plane\n"
     ]
    }
   ],
   "source": [
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}'\n",
    "                              for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设有一个张量 outputs，大小为 (2, 3)，表示有 2 个样本，每个样本有 3 个类别的预测结果。这个张量可以表示为一个矩阵，每行代表一个样本的预测结果。\n",
    "\n",
    "plaintext\n",
    "Copy code\n",
    "outputs = [[0.1, 0.8, 0.3],\n",
    "           [0.5, 0.2, 0.7]]\n",
    "现在，我们使用 torch.max(outputs, 1) 来获取每个样本预测结果中的最大值及其对应的索引。在这里，我们指定维度为 1，即沿着第二个维度进行操作，因为我们想要对每个样本的预测结果进行操作。\n",
    "\n",
    "首先，我们获取每个样本预测结果中的最大值及其对应的索引：\n",
    "\n",
    "对于第一个样本 [0.1, 0.8, 0.3]，最大值是 0.8，对应的索引是 1；\n",
    "对于第二个样本 [0.5, 0.2, 0.7]，最大值是 0.7，对应的索引是 2。\n",
    "因此，torch.max(outputs, 1) 返回的结果是一个元组 (max_values, max_indices)：\n",
    "\n",
    "max_values 是一个张量，包含每个样本预测结果中的最大值，即 [0.8, 0.7]；\n",
    "max_indices 是一个张量，包含每个样本预测结果中最大值的索引，即 [1, 2]。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "classes[predicted[j]] 这行代码的作用是将模型对前4个样本的预测结果打印出来，并以字符串形式展示。让我用之前的例子来解释一下：\n",
    "\n",
    "假设 classes 是一个包含类别名称的列表，比如 ['猫', '狗', '鸟']。而 predicted 是一个张量，包含了模型对前4个样本的预测结果的索引。\n",
    "\n",
    "假设 predicted 的值是 [1, 2, 0, 1]，那么这意味着：\n",
    "\n",
    "第一个样本被预测为第二类（索引为1），即狗；\n",
    "第二个样本被预测为第三类（索引为2），即鸟；\n",
    "第三个样本被预测为第一类（索引为0），即猫；\n",
    "第四个样本被预测为第二类（索引为1），即狗。\n",
    "因此，' '.join(f'{classes[predicted[j]]:5s}' for j in range(4)) 这部分代码会生成一个字符串，其中包含了模型对这4个样本的预测结果的类别名称，每个名称占据5个字符的宽度，以确保对齐。最终的字符串会以空格分隔每个预测结果，并在前面加上 \"Predicted: \"，以便更好地显示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### range(4) 中的 4 是指代批处理中的样本数量 ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们看看该网络在整个数据集上的表现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[76], line 10\u001B[0m\n\u001B[1;32m      8\u001B[0m images, labels \u001B[38;5;241m=\u001B[39m data\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# 将图像数据输入到神经网络模型 net 中，得到模型的输出结果\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mnet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# the class with the highest energy is what we choose as prediction\u001B[39;00m\n\u001B[1;32m     12\u001B[0m _, predicted \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmax(outputs\u001B[38;5;241m.\u001B[39mdata, \u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[71], line 17\u001B[0m, in \u001B[0;36mNet.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m---> 17\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpool(F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m))\n\u001B[1;32m     18\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpool(F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv2(x)))\n\u001B[1;32m     19\u001B[0m     x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mflatten(x, \u001B[38;5;241m1\u001B[39m) \u001B[38;5;66;03m# flatten all dimensions except batch\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py:460\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    459\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 460\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py:456\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[0;34m(self, input, weight, bias)\u001B[0m\n\u001B[1;32m    452\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m    453\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[1;32m    454\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[1;32m    455\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[0;32m--> 456\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    457\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "# correct 用于记录模型在测试数据集上正确分类的样本数量，total 则用于记录测试数据集中总共的样本数量\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    # testloader 它提供了一个数据迭代器，可以用于遍历测试数据集中的每个批次\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        # 将图像数据输入到神经网络模型 net 中，得到模型的输出结果\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on GPU #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(6, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请记住，您还必须将每一步的输入和目标发送到 GPU："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels = data[0].to(device), data[1].to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与 CPU 相比，为什么我没有发现明显的速度提升？因为您的网络太小了。\n",
    "\n",
    "练习： 尝试增加网络的宽度（第一个 nn.Conv2d 的参数 2 和第二个 nn.Conv2d 的参数 1 - 它们必须是相同的数字），看看会有什么样的加速效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "          # self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 6, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将第二个卷积层的输出通道数也设置为 6，这样就增加了网络的宽度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instrumentation et évaluation \"en continu\" du système ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la partie entraînement du réseau CNN, lister les différentes couches et sous-couches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La couche de convolution : ###\n",
    "La couche de convolution applique un ensemble de filtres convolutifs aux images en entrée, chacun d'entre eux activant certaines caractéristiques des images.\n",
    "- `self.conv1` (Première couche de convolution)\n",
    "- `self.conv2` (Deuxième couche de convolution)\n",
    "\n",
    "### La couche de pooling : ###\n",
    "L'opération de pooling consiste à réduire la taille des images, tout en préservant leurs caractéristiques importantes.\n",
    "- `self.pool` qui est utilisée après chaque couche de convolution\n",
    "\n",
    "### La couche de correction ReLU ###\n",
    "La couche de correction ReLU favorise un apprentissage plus rapide et plus efficace en remplaçant les valeurs négatives par des zéros et en conservant les valeurs positives.\n",
    "- Fonctions d'activation ReLU `F.relu` qui introduit de la non-linéarité dans le réseau.\n",
    "\n",
    "### Les couches entièrement connectées : ###\n",
    "Chaque couche entièrement connectée effectue une transformation linéaire suivie d'une activation ReLU, qui introduit de la non-linéarité dans le réseau.  Du coup, dans chacune des couches entièrement connectées (`self.fc1`, `self.fc2` et `self.fc3`), il y a une sous-couche linéaire suivie d'une sous-couche non linéaire. Ces couches sont responsables de la combinaison des caractéristiques extraites par les couches de convolution précédentes pour effectuer la tâche de classification finale.\n",
    "\n",
    "`self.fc1` (Première couche entièrement connectée)\n",
    "- Sous-couche linéaire : C'est la première étape de la couche. Elle effectue une transformation linéaire des caractéristiques d'entrée.\n",
    "- Sous-couche non linéaire : Après la transformation linéaire, une activation ReLU est appliquée. Cela introduit de la non-linéarité dans la sortie de la couche.\n",
    "\n",
    "`self.fc2` (Deuxième couche entièrement connectée)\n",
    "- Sous-couche linéaire : Comme pour la première couche, cette couche effectue une transformation linéaire des caractéristiques.\n",
    "- Sous-couche non linéaire : Suite à la transformation linéaire, une activation ReLU est appliquée.\n",
    "\n",
    "`self.fc3` (Couche de sortie)\n",
    "- Sous-couche linéaire : Il s'agit de la dernière transformation linéaire qui produit la sortie finale du réseau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donner la taille des différents tenseurs de données Xn et de poids Wn le long du calcul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}