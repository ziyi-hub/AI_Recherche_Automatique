{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/lib/oar/.batch_job_bashrc: line 5: /home/ziwang/.bashrc: No such file or directory\n",
      "Requirement already satisfied: torchsummary in ./.local/lib/python3.9/site-packages (1.5.1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import time\n",
    "!pip install torchsummary\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_epoch = 0  # 定义已经遍历数据集的次数\n",
    "BATCH_SIZE = 128      #批处理尺寸(batch_size)\n",
    "LR = 0.01        #学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train) \n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2) \n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "# classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def count_maxpool_operations(self, input, output, pool_layer, out_channels):\n",
    "        # input output 的维度均为 N,C,H,W\n",
    "        kernel_maxpooling = pool_layer.kernel_size\n",
    "        stride = pool_layer.stride\n",
    "        padding = pool_layer.padding\n",
    "        output_height = output.shape[2]\n",
    "        output_width = output.shape[3]\n",
    "        out_channels =  output.shape[1]\n",
    "        num_max = output_height * output_width * (kernel_maxpooling**2 -1) * out_channels\n",
    "        return num_max\n",
    "\n",
    "    def count_conv_operations(self, input, output, output_pooled, conv_layer, pool_layer):\n",
    "        # batch_size = input.size(0)\n",
    "        out_channels, in_channels = output.size(1), conv_layer.in_channels\n",
    "        output_height, output_width = output.size(2), output.size(3)\n",
    "        filter_size = conv_layer.kernel_size[0]\n",
    "        stride = conv_layer.stride[0]\n",
    "        padding = conv_layer.padding[0]\n",
    "        # Compute number of operations for convolution\n",
    "        # print(str(output_height) + \"*\" +  str(output_width) + \"*\" + str(in_channels) + \"*\" + str(filter_size ** 2) + \"*\" + str(out_channels))\n",
    "        num_mults = output_height * output_width * in_channels * filter_size ** 2 * out_channels\n",
    "        num_adds = output_height * output_width * in_channels * filter_size ** 2 * out_channels\n",
    "        # num_maxs = output_height * output_width * out_channels\n",
    "        # print(\"num_mults\" + str(num_mults))\n",
    "        num_maxs = self.count_maxpool_operations(output, output_pooled, pool_layer, out_channels)\n",
    "        # print(\"num_maxs\" + str(num_maxs))\n",
    "        total_ops = num_mults + num_adds + num_maxs\n",
    "        return num_mults, num_adds, num_maxs, total_ops\n",
    "\n",
    "    def count_operations(self, x):\n",
    "        conv1_out = self.conv1(x)\n",
    "        conv1_out_pooled = self.pool(F.relu(conv1_out))  # Apply max pooling after the first convolution\n",
    "        conv2_out = self.conv2(conv1_out_pooled)\n",
    "        conv2_out_pooled = self.pool(F.relu(conv2_out))  # Apply max pooling after the first convolution\n",
    "        # Count operations for convolutional layer 1\n",
    "        conv1_ops = self.count_conv_operations(x, conv1_out, conv1_out_pooled, self.conv1, self.pool)\n",
    "        # Count operations for convolutional layer 2\n",
    "        conv2_ops = self.count_conv_operations(conv1_out_pooled, conv2_out, conv2_out_pooled, self.conv2, self.pool)\n",
    "        return conv1_ops, conv2_ops        \n",
    "\n",
    "    def count_fc_operations(self, input, fc_layer):\n",
    "        # Get the number of input features for the fully connected layer\n",
    "        in_features = fc_layer.in_features\n",
    "        # Get the number of output features for the fully connected layer\n",
    "        out_features = fc_layer.out_features    \n",
    "        # Compute number of operations for fully connected layer\n",
    "        # print(str(out_features) + \" * \" + str(in_features))\n",
    "        num_mults = out_features * in_features\n",
    "        num_adds = out_features * in_features\n",
    "        num_maxs = 0    \n",
    "        total_ops = num_mults + num_adds\n",
    "        return num_mults, num_adds, num_maxs, total_ops\n",
    "\n",
    "    def count_total_operations(self, x):\n",
    "        conv1_ops, conv2_ops = self.count_operations(x)\n",
    "        fc1_ops = self.count_fc_operations(x, self.fc1)\n",
    "        fc2_ops = self.count_fc_operations(x, self.fc2)\n",
    "        fc3_ops = self.count_fc_operations(x, self.fc3)\n",
    "        total_ops = sum(op[3] for op in [conv1_ops, conv2_ops, fc1_ops, fc2_ops, fc3_ops])\n",
    "        return total_ops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(net, dataloader, criterion, epoch, device, iteration_num, i):\n",
    "    # correct_pred = {classname: 0 for classname in classes}\n",
    "    # total_pred = {classname: 0 for classname in classes}\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        eval_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        iteration_num = 0\n",
    "        batch_evaluate_ops = 0\n",
    "        if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "        epoch_evaluate_start = time.time()\n",
    "        \n",
    "        for data in dataloader:\n",
    "            net.eval()\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            batch_evaluate_ops = net.count_total_operations(images)\n",
    "            batch_evaluate_ops = images.shape[0] * batch_evaluate_ops\n",
    "            \n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            eval_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            \n",
    "            # for label, prediction in zip(labels, predicted):\n",
    "            #     if label == prediction.item():\n",
    "            #         correct_pred[classes[label]] += 1\n",
    "            #     total_pred[classes[label]] += 1\n",
    "            \n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            iteration_num += 1\n",
    "            \n",
    "        accuracy = 100. * correct / total\n",
    "        eval_loss /= len(dataloader)\n",
    "        if(epoch == 0):\n",
    "            epoch_evaluate_ops = batch_evaluate_ops * iteration_num \n",
    "        if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "        epoch_evaluate_time = time.time() - epoch_evaluate_start\n",
    "\n",
    "    # for classname, correct_count in correct_pred.items():\n",
    "    #     af = 100 * float(correct_count) / total_pred[classname]\n",
    "    #     print(f'Accuracy for class: {classname:5s} is {af:.1f} %')\n",
    "    \n",
    "    return accuracy, eval_loss, batch_evaluate_ops, epoch_evaluate_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch_nums, net, trainloader, testloader, optimizer, criterion, device):\n",
    "    if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "    total_time_start = time.time()\n",
    "    for epoch in range(epoch_nums):\n",
    "        net.train()\n",
    "        sum_loss = 0.0\n",
    "        correct = 0.0\n",
    "        total = 0.0\n",
    "        iteration_num = 0\n",
    "\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if epoch == 0 & i == 0:\n",
    "                batch_train_ops = net.count_total_operations(inputs)\n",
    "                batch_train_ops = inputs.shape[0] * batch_train_ops\n",
    "            if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "            batch_train_start = time.time()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item()\n",
    "\n",
    "            if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "            batch_train_time = time.time() - batch_train_start\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            # batch_train_ops_per_second = batch_train_ops / batch_train_time\n",
    "            iteration_num += 1\n",
    "            \n",
    "        if(epoch == 0):\n",
    "            epoch_train_ops = batch_train_ops * iteration_num\n",
    "        \n",
    "        accuracy, eval_loss, batch_evaluate_ops, epoch_evaluate_time = evaluate_model(net, testloader, criterion, epoch, device, iteration_num, i)\n",
    "        print('[Epoch:%d] Validation Acc: %.3f%% | Loss: %.3f%% | Ops: %d | Time: %.6fs ' % (\n",
    "            epoch + 1, \n",
    "            accuracy, \n",
    "            eval_loss,\n",
    "            batch_evaluate_ops, \n",
    "            epoch_evaluate_time,\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:1] Validation Acc: 10.320% | Loss: 2.302% | Ops: 20930688 | Time: 1.060341s \n",
      "[Epoch:2] Validation Acc: 15.490% | Loss: 2.298% | Ops: 20930688 | Time: 0.851083s \n",
      "[Epoch:3] Validation Acc: 15.250% | Loss: 2.273% | Ops: 20930688 | Time: 1.492649s \n",
      "[Epoch:4] Validation Acc: 21.950% | Loss: 2.172% | Ops: 20930688 | Time: 0.821721s \n",
      "[Epoch:5] Validation Acc: 26.130% | Loss: 2.022% | Ops: 20930688 | Time: 1.394150s \n",
      "[Epoch:6] Validation Acc: 29.920% | Loss: 1.920% | Ops: 20930688 | Time: 1.213524s \n",
      "[Epoch:7] Validation Acc: 33.520% | Loss: 1.830% | Ops: 20930688 | Time: 1.499590s \n",
      "[Epoch:8] Validation Acc: 37.140% | Loss: 1.744% | Ops: 20930688 | Time: 1.192477s \n",
      "[Epoch:9] Validation Acc: 38.000% | Loss: 1.678% | Ops: 20930688 | Time: 1.194107s \n",
      "[Epoch:10] Validation Acc: 39.730% | Loss: 1.632% | Ops: 20930688 | Time: 1.312217s \n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)    \n",
    "\n",
    "epoch_nums = 10  # or any number of epochs you want\n",
    "train(epoch_nums, net, trainloader, testloader, optimizer, criterion, device)\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 6, 28, 28]             456\n",
      "         MaxPool2d-2            [-1, 6, 14, 14]               0\n",
      "            Conv2d-3           [-1, 16, 10, 10]           2,416\n",
      "         MaxPool2d-4             [-1, 16, 5, 5]               0\n",
      "            Linear-5                  [-1, 120]          48,120\n",
      "            Linear-6                   [-1, 84]          10,164\n",
      "            Linear-7                   [-1, 10]             850\n",
      "================================================================\n",
      "Total params: 62,006\n",
      "Trainable params: 62,006\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.06\n",
      "Params size (MB): 0.24\n",
      "Estimated Total Size (MB): 0.31\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "input_size = (3, 32, 32)\n",
    "net = net.to(device)\n",
    "summary(net, input_size = input_size)\n",
    "# Number of Parameters CONVOL1 = out_channels × (in_channels × kernel_size ** 2 + 1) = 6 * (3 * 25 + 1) = 6 * 76 = 456\n",
    "# Taille de sortie après 1 layer convolution: ((size_input - kernel_size) + 1 // stride) =  (32 - 5) + 1 = 28\n",
    "# Maxpooling (2 * 2) = Taille de sortie // 2 = 28 // 2 = 14\n",
    "# MaxPool2d 层不包含可学习的参数 = 0\n",
    "# Taille de sortie après 2 layer convolution: ((14 - 5) + 1) // 1 = 10\n",
    "# Number of Parameters CONVOL2 = out_channels × (in_channels × kernel_size ** 2 + 1) = 16 * (6 * 25 + 1) = 2416\n",
    "# Maxpooling (2 * 2) = Taille de sortie // 2 = 10 // 2 = 5\n",
    "# Number of Parameters FC1 =(in_channels + 1) × out_channels = ((16 * 5 * 5) + 1) * 120 = 48120\n",
    "# Number of Parameters FC2 =(in_channels + 1) × out_channels = ((120) + 1) * 84 = 10164\n",
    "# Number of Parameters FC3 =(in_channels + 1) × out_channels = ((84) + 1) * 10 = 850\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, inchannel, outchannel, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.left = nn.Sequential(\n",
    "            nn.Conv2d(inchannel, outchannel, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(outchannel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(outchannel, outchannel, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(outchannel)\n",
    "        )\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or inchannel != outchannel:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(inchannel, outchannel, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(outchannel)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.left(x)\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, ResidualBlock, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inchannel = 64\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layer1 = self.make_layer(ResidualBlock, 64,  2, stride=1)\n",
    "        #self.layer2 = self.make_layer(ResidualBlock, 128, 2, stride=2)\n",
    "        #self.layer3 = self.make_layer(ResidualBlock, 256, 2, stride=2)\n",
    "        #self.layer4 = self.make_layer(ResidualBlock, 512, 2, stride=2)\n",
    "        #self.fc = nn.Linear(512, num_classes)\n",
    "        #self.fc = nn.Linear(128, num_classes)\n",
    "        self.fc = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def make_layer(self, block, channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)   #strides=[1,1]\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.inchannel, channels, stride))\n",
    "            self.inchannel = channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.layer1(out)\n",
    "        #out = self.layer2(out)\n",
    "        #out = self.layer3(out)\n",
    "        #out = self.layer4(out)\n",
    "        #out = F.avg_pool2d(out, 4)\n",
    "        #out = F.avg_pool2d(out, 16)\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet18():\n",
    "    return ResNet(ResidualBlock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(My_AlexNet, self).__init__()\n",
    "        # 特征提取\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3,out_channels=16,kernel_size=5,stride=1,padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3,stride=2,padding=0),\n",
    "            nn.Conv2d(in_channels=16,out_channels=48,kernel_size=3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(48),\n",
    "            nn.MaxPool2d(kernel_size=3,stride=2,padding=0),\n",
    "            nn.Conv2d(in_channels=48,out_channels=64,kernel_size=3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64,out_channels=48,kernel_size=3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3,stride=2,padding=0),\n",
    "        )\n",
    "        # 全连接层\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=3*3*48,out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,10)\n",
    "        )\n",
    "\n",
    "    # 前向算法\n",
    "    def forward(self,x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        result = self.classifier(x)\n",
    "        return result\n",
    "\n",
    "\n",
    "    def count_maxpool_operations(self, input, output, pool_layer):\n",
    "        if pool_layer:\n",
    "            kernel_size = pool_layer.kernel_size\n",
    "            stride = pool_layer.stride\n",
    "            padding = pool_layer.padding\n",
    "            output_height = output.shape[2]\n",
    "            output_width = output.shape[3]\n",
    "            out_channels = output.shape[1]\n",
    "            num_max = ((output_height + 2 * padding - kernel_size) // stride + 1) * ((output_width + 2 * padding - kernel_size) // stride + 1) * (kernel_size ** 2 - 1) * out_channels  \n",
    "        else:\n",
    "            num_max = 0\n",
    "        return num_max\n",
    "             \n",
    "    def count_conv_operations(self, input, output, output_pooled, conv_layer, pool_layer):\n",
    "        out_channels, in_channels = output.size(1), conv_layer.in_channels\n",
    "        output_height, output_width = output.size(2), output.size(3)\n",
    "        filter_size = conv_layer.kernel_size[0]\n",
    "        stride = conv_layer.stride[0]\n",
    "        padding = conv_layer.padding[0]\n",
    "        num_mults = output_height * output_width * in_channels * filter_size ** 2 * out_channels\n",
    "        num_adds = output_height * output_width * in_channels * filter_size ** 2 * out_channels\n",
    "        num_maxs = self.count_maxpool_operations(output, output_pooled, pool_layer)\n",
    "        if pool_layer:\n",
    "            num_maxs = self.count_maxpool_operations(output, output_pooled, pool_layer)\n",
    "        else:\n",
    "            num_maxs = 0\n",
    "        total_ops = num_mults + num_adds + num_maxs\n",
    "        return num_mults, num_adds, num_maxs, total_ops\n",
    "\n",
    "    def count_operations(self, x):\n",
    "        conv1_out = self.features[0](x)\n",
    "        conv1_out_pooled = self.features[2](F.relu(conv1_out))\n",
    "        conv2_out = self.features[3](conv1_out_pooled)\n",
    "        conv2_out_pooled = self.features[6](F.relu(conv2_out))\n",
    "        \n",
    "        conv3_out = self.features[7](conv2_out_pooled)\n",
    "        conv4_out = self.features[9](F.relu(conv3_out))\n",
    "        conv5_out = self.features[11](F.relu(conv4_out))\n",
    "        conv5_out_pooled = self.features[13](F.relu(conv5_out))\n",
    "        \n",
    "        conv1_ops = self.count_conv_operations(x, conv1_out, conv1_out_pooled, self.features[0], self.features[2])\n",
    "        conv2_ops = self.count_conv_operations(conv1_out_pooled, conv2_out, conv2_out_pooled, self.features[3], self.features[6])\n",
    "        conv3_ops = self.count_conv_operations(conv2_out_pooled, conv3_out, conv3_out, self.features[7], None)\n",
    "        conv4_ops = self.count_conv_operations(conv3_out, conv4_out, conv4_out, self.features[9], None)\n",
    "        conv5_ops = self.count_conv_operations(conv4_out, conv5_out, conv5_out_pooled, self.features[11], self.features[13])\n",
    "        return conv1_ops, conv2_ops, conv3_ops, conv4_ops, conv5_ops\n",
    "\n",
    "\n",
    "    def count_fc_operations(self, input, fc_layer):\n",
    "        in_features = fc_layer.in_features\n",
    "        out_features = fc_layer.out_features    \n",
    "        num_mults = out_features * in_features\n",
    "        num_adds = out_features * in_features\n",
    "        num_maxs = 0\n",
    "        total_ops = num_mults + num_adds\n",
    "        return num_mults, num_adds, num_maxs, total_ops\n",
    "    \n",
    "    def count_total_operations(self, x):\n",
    "        conv1_ops, conv2_ops, conv3_ops, conv4_ops, conv5_ops  = self.count_operations(x)\n",
    "        fc1_ops = self.count_fc_operations(x, self.classifier[0])\n",
    "        fc2_ops = self.count_fc_operations(x, self.classifier[3])\n",
    "        fc3_ops = self.count_fc_operations(x, self.classifier[5])\n",
    "        total_ops = sum(op[3] for op in [conv1_ops, conv2_ops, conv3_ops, conv4_ops, conv5_ops, fc1_ops, fc2_ops, fc3_ops])\n",
    "        return total_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:1] Validation Acc: 31.850% | Loss: 1.780% | Ops: 236091392 | Time: 1.190848s \n",
      "[Epoch:2] Validation Acc: 50.120% | Loss: 1.343% | Ops: 236091392 | Time: 1.169844s \n",
      "[Epoch:3] Validation Acc: 55.780% | Loss: 1.219% | Ops: 236091392 | Time: 1.467693s \n",
      "[Epoch:4] Validation Acc: 58.070% | Loss: 1.168% | Ops: 236091392 | Time: 1.405068s \n",
      "[Epoch:5] Validation Acc: 64.850% | Loss: 0.981% | Ops: 236091392 | Time: 1.443400s \n",
      "[Epoch:6] Validation Acc: 67.210% | Loss: 0.942% | Ops: 236091392 | Time: 1.450630s \n",
      "[Epoch:7] Validation Acc: 64.420% | Loss: 1.037% | Ops: 236091392 | Time: 1.300340s \n",
      "[Epoch:8] Validation Acc: 69.470% | Loss: 0.892% | Ops: 236091392 | Time: 1.394092s \n",
      "[Epoch:9] Validation Acc: 70.990% | Loss: 0.830% | Ops: 236091392 | Time: 1.186645s \n",
      "[Epoch:10] Validation Acc: 73.370% | Loss: 0.773% | Ops: 236091392 | Time: 1.347919s \n",
      "[Epoch:11] Validation Acc: 72.960% | Loss: 0.794% | Ops: 236091392 | Time: 1.484970s \n",
      "[Epoch:12] Validation Acc: 74.120% | Loss: 0.775% | Ops: 236091392 | Time: 1.265448s \n",
      "[Epoch:13] Validation Acc: 76.810% | Loss: 0.688% | Ops: 236091392 | Time: 1.308826s \n",
      "[Epoch:14] Validation Acc: 75.670% | Loss: 0.750% | Ops: 236091392 | Time: 1.174745s \n",
      "[Epoch:15] Validation Acc: 76.290% | Loss: 0.695% | Ops: 236091392 | Time: 1.175402s \n",
      "[Epoch:16] Validation Acc: 76.530% | Loss: 0.703% | Ops: 236091392 | Time: 1.174802s \n",
      "[Epoch:17] Validation Acc: 78.520% | Loss: 0.639% | Ops: 236091392 | Time: 1.213814s \n",
      "[Epoch:18] Validation Acc: 77.790% | Loss: 0.657% | Ops: 236091392 | Time: 1.396655s \n",
      "[Epoch:19] Validation Acc: 78.840% | Loss: 0.617% | Ops: 236091392 | Time: 1.211592s \n",
      "[Epoch:20] Validation Acc: 78.950% | Loss: 0.632% | Ops: 236091392 | Time: 1.400788s \n",
      "[Epoch:21] Validation Acc: 79.770% | Loss: 0.601% | Ops: 236091392 | Time: 1.281231s \n",
      "[Epoch:22] Validation Acc: 79.290% | Loss: 0.620% | Ops: 236091392 | Time: 1.448276s \n",
      "[Epoch:23] Validation Acc: 78.720% | Loss: 0.626% | Ops: 236091392 | Time: 1.455136s \n",
      "[Epoch:24] Validation Acc: 79.510% | Loss: 0.609% | Ops: 236091392 | Time: 1.422395s \n",
      "[Epoch:25] Validation Acc: 78.330% | Loss: 0.650% | Ops: 236091392 | Time: 1.159810s \n",
      "[Epoch:26] Validation Acc: 79.720% | Loss: 0.602% | Ops: 236091392 | Time: 1.385304s \n",
      "[Epoch:27] Validation Acc: 80.210% | Loss: 0.581% | Ops: 236091392 | Time: 1.355666s \n",
      "[Epoch:28] Validation Acc: 79.930% | Loss: 0.575% | Ops: 236091392 | Time: 1.371379s \n",
      "[Epoch:29] Validation Acc: 79.910% | Loss: 0.620% | Ops: 236091392 | Time: 0.833308s \n",
      "[Epoch:30] Validation Acc: 80.330% | Loss: 0.583% | Ops: 236091392 | Time: 1.289743s \n",
      "[Epoch:31] Validation Acc: 80.840% | Loss: 0.576% | Ops: 236091392 | Time: 0.832051s \n",
      "[Epoch:32] Validation Acc: 80.070% | Loss: 0.602% | Ops: 236091392 | Time: 1.405905s \n",
      "[Epoch:33] Validation Acc: 81.040% | Loss: 0.559% | Ops: 236091392 | Time: 1.166058s \n",
      "[Epoch:34] Validation Acc: 81.120% | Loss: 0.567% | Ops: 236091392 | Time: 1.211648s \n",
      "[Epoch:35] Validation Acc: 80.900% | Loss: 0.580% | Ops: 236091392 | Time: 1.426403s \n",
      "[Epoch:36] Validation Acc: 81.980% | Loss: 0.543% | Ops: 236091392 | Time: 1.430370s \n",
      "[Epoch:37] Validation Acc: 81.520% | Loss: 0.568% | Ops: 236091392 | Time: 1.366601s \n",
      "[Epoch:38] Validation Acc: 81.250% | Loss: 0.565% | Ops: 236091392 | Time: 1.306166s \n",
      "[Epoch:39] Validation Acc: 82.350% | Loss: 0.526% | Ops: 236091392 | Time: 1.486563s \n",
      "[Epoch:40] Validation Acc: 80.980% | Loss: 0.561% | Ops: 236091392 | Time: 1.511420s \n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "#net = ResNet18().to(device)\n",
    "net = My_AlexNet().to(device)\n",
    "\n",
    "# 定义损失函数和优化方式\n",
    "criterion = nn.CrossEntropyLoss()  #损失函数为交叉熵，多用于多分类问题\n",
    "optimizer = optim.SGD(net.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4) #优化方式为mini-batch momentum-SGD，并采用L2正则化（权重衰减）\n",
    "\n",
    "epoch_nums = 40  # or any number of epochs you want\n",
    "train(epoch_nums, net, trainloader, testloader, optimizer, criterion, device)\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
