{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/lib/oar/.batch_job_bashrc: line 5: /home/ziwang/.bashrc: No such file or directory\n",
      "Requirement already satisfied: torchsummary in ./.local/lib/python3.9/site-packages (1.5.1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import time\n",
    "!pip install torchsummary\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_epoch = 0\n",
    "BATCH_SIZE = 128\n",
    "LR = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train) \n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2) \n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "# classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instrumentation et évaluation \"en continu\" du système #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifier les fonctions pour calculer à chaque étape le nombre d'opérations flottantes effectuées, séparément pour les additions, les multiplications, les maximums et le total. ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def count_maxpool_operations(self, input, output, pool_layer, out_channels):\n",
    "        kernel_maxpooling = pool_layer.kernel_size\n",
    "        stride = pool_layer.stride\n",
    "        padding = pool_layer.padding\n",
    "        output_height = output.shape[2]\n",
    "        output_width = output.shape[3]\n",
    "        out_channels =  output.shape[1]\n",
    "        num_max = output_height * output_width * (kernel_maxpooling**2 -1) * out_channels\n",
    "        return num_max\n",
    "\n",
    "    def count_conv_operations(self, input, output, output_pooled, conv_layer, pool_layer):\n",
    "        out_channels, in_channels = output.size(1), conv_layer.in_channels\n",
    "        output_height, output_width = output.size(2), output.size(3)\n",
    "        filter_size = conv_layer.kernel_size[0]\n",
    "        stride = conv_layer.stride[0]\n",
    "        padding = conv_layer.padding[0]\n",
    "        num_mults = output_height * output_width * in_channels * filter_size ** 2 * out_channels\n",
    "        num_adds = output_height * output_width * in_channels * filter_size ** 2 * out_channels\n",
    "        num_maxs = self.count_maxpool_operations(output, output_pooled, pool_layer, out_channels)\n",
    "        total_ops = num_mults + num_adds + num_maxs\n",
    "        return num_mults, num_adds, num_maxs, total_ops\n",
    "\n",
    "    def count_operations(self, x):\n",
    "        conv1_out = self.conv1(x)\n",
    "        conv1_out_pooled = self.pool(F.relu(conv1_out))\n",
    "        conv2_out = self.conv2(conv1_out_pooled)\n",
    "        conv2_out_pooled = self.pool(F.relu(conv2_out))\n",
    "        conv1_ops = self.count_conv_operations(x, conv1_out, conv1_out_pooled, self.conv1, self.pool)\n",
    "        conv2_ops = self.count_conv_operations(conv1_out_pooled, conv2_out, conv2_out_pooled, self.conv2, self.pool)\n",
    "        return conv1_ops, conv2_ops        \n",
    "\n",
    "    def count_fc_operations(self, input, fc_layer):\n",
    "        in_features = fc_layer.in_features\n",
    "        out_features = fc_layer.out_features\n",
    "        num_mults = out_features * in_features\n",
    "        num_adds = out_features * in_features\n",
    "        num_maxs = 0    \n",
    "        total_ops = num_mults + num_adds\n",
    "        return num_mults, num_adds, num_maxs, total_ops\n",
    "\n",
    "    def count_total_operations(self, x):\n",
    "        conv1_ops, conv2_ops = self.count_operations(x)\n",
    "        fc1_ops = self.count_fc_operations(x, self.fc1)\n",
    "        fc2_ops = self.count_fc_operations(x, self.fc2)\n",
    "        fc3_ops = self.count_fc_operations(x, self.fc3)\n",
    "        total_ops = sum(op[3] for op in [conv1_ops, conv2_ops, fc1_ops, fc2_ops, fc3_ops])\n",
    "        return total_ops\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dans la partie entraînement du réseau CNN, lister les différentes couches et sous-couches. ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La couche de convolution : ###\n",
    "La couche de convolution applique un ensemble de filtres convolutifs aux images en entrée, chacun d'entre eux activant certaines caractéristiques des images. \n",
    "\n",
    "- `self.conv1` Une première couche de convolution prend en entrée des images RGB et produit 6 filtres de convolution de taille 5×5 sans padding.\n",
    "- `self.conv2` Une deuxième couche de convolution prend 6 entrées et produit 16 filtres de convolution de taille 5×5 sans padding.\n",
    "\n",
    "### Les couches entièrement connectées : ###\n",
    "Chaque couche entièrement connectée effectue une transformation linéaire suivie d'une activation ReLU, qui introduit de la non-linéarité dans le réseau.  Du coup, dans chacune des couches entièrement connectées (`self.fc1`, `self.fc2` et `self.fc3`), il y a une sous-couche linéaire suivie d'une sous-couche non linéaire. Ces couches sont responsables de la combinaison des caractéristiques extraites par les couches de convolution précédentes pour effectuer la tâche de classification finale.\n",
    "\n",
    "`self.fc1`: \n",
    "- Il y a 120 neurones dans la couche entièrement connectée, chacun connecté à une entrée de taille 400.\n",
    "- Sous-couche linéaire : Elle effectue une transformation linéaire des caractéristiques d'entrée.\n",
    "- Sous-couche non linéaire : Suite à la transformation linéaire, une activation ReLU est appliquée. Cela introduit de la non-linéarité dans la sortie de la couche.\n",
    "\n",
    "`self.fc2`:\n",
    "- 84 neurones dans la deuxième couche entièrement connectée, chacun connecté aux 120 neurones de la couche précédente.\n",
    "- Sous-couche linéaire : Elle effectue une transformation linéaire des caractéristiques d'entrée.\n",
    "- Sous-couche non linéaire : Suite à la transformation linéaire, une activation ReLU est appliquée. Cela introduit de la non-linéarité dans la sortie de la couche.\n",
    "\n",
    "`self.fc3`:\n",
    "- 10 neurones dans la dernière couche entièrement connectée, chacun connecté aux 84 neurones de la couche précédente.\n",
    "- Sous-couche linéaire : Il s'agit de la dernière transformation linéaire qui produit la sortie finale du réseau sans RELU.\n",
    "\n",
    "### La couche de pooling : ###\n",
    "L'opération de pooling consiste à réduire la taille des images, tout en préservant leurs caractéristiques importantes. Elle est utilisée après chaque couche de convolution.\n",
    "- `self.pool` Cette couche en effectuant une opération de max pooling avec une fenêtre de taille 2x2 et un pas de 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Donner la taille des différents tenseurs de données Xn et de poids Wn le long du calcul. ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Layer 1 (conv1): ###\n",
    "- X1 = 3×32×32 = 3072\n",
    "- Poids W1:\n",
    "    - [6, 3, 5, 5]: Poids de convolution: 6×3×5×5=450\n",
    "    - Bias: [6]\n",
    "\n",
    "### Convolutional Layer 2 (conv2): ###\n",
    "- X2 = 6×14×14 = 1176\n",
    "- Poids W2:\n",
    "    - [16, 6, 5, 5]: Poids de convolution: 16×6×5×5=2400\n",
    "    - Bias: [16]\n",
    "\n",
    "### Fully Connected Layer 1 (fc1): ###\n",
    "- X3 = 16×5×5 = 400\n",
    "- Poids W3:\n",
    "    - [120, 400]: Poids de convolution: 120×400 = 48000\n",
    "    - Bias: [120]\n",
    "\n",
    "### Fully Connected Layer 2 (fc2): ###\n",
    "- X4 = 120\n",
    "- Poids W4:\n",
    "    - [84, 120]: Poids de convolution: 84×120=10080\n",
    "    - Bias: [84]\n",
    "  \n",
    "### Fully Connected Layer 3 (fc3): ###\n",
    "- X5 = 84\n",
    "- Poids W5:\n",
    "    - [10, 84]: Poids de convolution: 10×84=840\n",
    "    - Bias: [10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifier le programme pour faire l'évaluation après chaque époque et aussi avant la première (faire une fonction spécialisée). Supprimer les autres affichages intermédiaires. ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(net, dataloader, criterion, epoch, device, iteration_num, i):\n",
    "    # correct_pred = {classname: 0 for classname in classes}\n",
    "    # total_pred = {classname: 0 for classname in classes}\n",
    "    epoch_evaluate_ops = 0\n",
    "    with torch.no_grad(): \n",
    "        eval_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        iteration_num = 0\n",
    "        batch_evaluate_ops = 0\n",
    "        if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "        epoch_evaluate_start = time.time()\n",
    "        \n",
    "        for data in dataloader:\n",
    "            net.eval()\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            batch_evaluate_ops = net.count_total_operations(images)\n",
    "            batch_evaluate_ops = images.shape[0] * batch_evaluate_ops # batch_size * total_operation\n",
    "            epoch_evaluate_ops += batch_evaluate_ops\n",
    "            \n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            eval_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            \n",
    "            # for label, prediction in zip(labels, predicted):\n",
    "            #     if label == prediction.item():\n",
    "            #         correct_pred[classes[label]] += 1\n",
    "            #     total_pred[classes[label]] += 1\n",
    "            \n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            iteration_num += 1\n",
    "            \n",
    "        accuracy = 100. * correct / total\n",
    "        eval_loss /= len(dataloader)\n",
    "        if(epoch == 0):\n",
    "            epoch_evaluate_ops = batch_evaluate_ops * iteration_num\n",
    "        if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "        epoch_evaluate_time = time.time() - epoch_evaluate_start\n",
    "        epoch_evaluate_ops_per_second = epoch_evaluate_ops / epoch_evaluate_time\n",
    "\n",
    "\n",
    "    # for classname, correct_count in correct_pred.items():\n",
    "    #     af = 100 * float(correct_count) / total_pred[classname]\n",
    "    #     print(f'Accuracy for class: {classname:5s} is {af:.1f} %')\n",
    "    \n",
    "    return accuracy, eval_loss, batch_evaluate_ops, epoch_evaluate_time, epoch_evaluate_ops_per_second\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch_nums, net, trainloader, testloader, optimizer, criterion, device):\n",
    "    if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "    total_time_start = time.time()\n",
    "    for epoch in range(epoch_nums):\n",
    "        net.train()\n",
    "        sum_loss = 0.0\n",
    "        correct = 0.0\n",
    "        total = 0.0\n",
    "        iteration_num = 0\n",
    "\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if epoch == 0 & i == 0:\n",
    "                batch_train_ops = net.count_total_operations(inputs)\n",
    "                batch_train_ops = inputs.shape[0] * batch_train_ops\n",
    "            if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "            batch_train_start = time.time()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item()\n",
    "\n",
    "            if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "            batch_train_time = time.time() - batch_train_start\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            # batch_train_ops_per_second = batch_train_ops / batch_train_time\n",
    "            iteration_num += 1\n",
    "            \n",
    "        if(epoch == 0):\n",
    "            epoch_train_ops = batch_train_ops * iteration_num\n",
    "        \n",
    "        accuracy, eval_loss, batch_evaluate_ops, epoch_evaluate_time, epoch_evaluate_ops_per_second = evaluate_model(net, testloader, criterion, epoch, device, iteration_num, i)\n",
    "        print('[Epoch:%d] Validation Acc: %.3f%% | Loss: %.3f%% | Ops: %d | Time: %.6fs | Ops/Sec : %d ' % (\n",
    "            epoch + 1, \n",
    "            accuracy, \n",
    "            eval_loss,\n",
    "            batch_evaluate_ops, \n",
    "            epoch_evaluate_time,\n",
    "            epoch_evaluate_ops_per_second\n",
    "        ))\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, inchannel, outchannel, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inchannel, outchannel, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(outchannel)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(outchannel, outchannel, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(outchannel)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or inchannel != outchannel:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(inchannel, outchannel, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(outchannel)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self.make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.fc = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def make_layer(self, block, channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        inchannel = 64  # 初始化输入通道数\n",
    "        for stride in strides:\n",
    "            layers.append(block(inchannel, channels, stride))\n",
    "            inchannel = channels  # 更新输入通道数为当前通道数\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer1(out)\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "    def count_conv_operations(self, input, output, output_pooled, stride, padding, kernel_size):\n",
    "        out_channels = output.size(1)\n",
    "        output_height, output_width = output.size(2), output.size(3)\n",
    "        num_mults = output_height * output_width * input.size(1) * kernel_size ** 2 * out_channels\n",
    "        num_adds = output_height * output_width * input.size(1) * kernel_size ** 2 * out_channels\n",
    "        num_maxs = self.count_maxpool_operations(output, output_pooled)\n",
    "        total_ops = num_mults + num_adds + num_maxs\n",
    "        return num_mults, num_adds, num_maxs, total_ops\n",
    "\n",
    "\n",
    "\n",
    "    def count_maxpool_operations(self, output, output_pooled):\n",
    "        kernel_size = 2  # 例如，假设池化层的核大小为 2\n",
    "        stride = 2  # 假设步长也为 2\n",
    "        padding = 0  # 假设填充为 0\n",
    "        output_height = output.shape[2]\n",
    "        output_width = output.shape[3]\n",
    "        out_channels = output.shape[1]\n",
    "        num_max = ((output_height + 2 * padding - kernel_size) // stride + 1) * ((output_width + 2 * padding - kernel_size) // stride + 1) * (kernel_size ** 2 - 1) * out_channels\n",
    "        return num_max\n",
    "\n",
    "\n",
    "    def count_fc_operations(self, input, fc_layer):\n",
    "        in_features = fc_layer.in_features\n",
    "        out_features = fc_layer.out_features    \n",
    "        num_mults = out_features * in_features\n",
    "        num_adds = out_features * in_features\n",
    "        num_maxs = 0\n",
    "        total_ops = num_mults + num_adds\n",
    "        return num_mults, num_adds, num_maxs, total_ops\n",
    "\n",
    "    def count_total_operations(self, x):\n",
    "        conv1_out = self.conv1(x)\n",
    "        conv1_out_pooled = self.relu(self.bn1(conv1_out))\n",
    "        conv1_ops = self.count_conv_operations(x, conv1_out, conv1_out_pooled, self.conv1, None, kernel_size=3)\n",
    "\n",
    "        layer1_out = self.layer1(conv1_out_pooled)\n",
    "        layer1_out_pooled = F.avg_pool2d(layer1_out, 8)\n",
    "        layer1_ops = self.count_conv_operations(conv1_out_pooled, layer1_out, layer1_out_pooled, stride=1, padding=1, kernel_size=3)\n",
    "\n",
    "        fc_ops = self.count_fc_operations(x, self.fc)\n",
    "\n",
    "        total_ops = sum(op[3] for op in [conv1_ops, layer1_ops, fc_ops])\n",
    "        return total_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet18():\n",
    "    return ResNet(ResidualBlock, [2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "  \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3,out_channels=16,kernel_size=5,stride=1,padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3,stride=2,padding=0),\n",
    "            nn.Conv2d(in_channels=16,out_channels=48,kernel_size=3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(48),\n",
    "            nn.MaxPool2d(kernel_size=3,stride=2,padding=0),\n",
    "            nn.Conv2d(in_channels=48,out_channels=64,kernel_size=3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64,out_channels=48,kernel_size=3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3,stride=2,padding=0),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=3*3*48,out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,10)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        result = self.classifier(x)\n",
    "        return result\n",
    "\n",
    "\n",
    "    def count_maxpool_operations(self, input, output, pool_layer):\n",
    "        if pool_layer:\n",
    "            kernel_size = pool_layer.kernel_size\n",
    "            stride = pool_layer.stride\n",
    "            padding = pool_layer.padding\n",
    "            output_height = output.shape[2]\n",
    "            output_width = output.shape[3]\n",
    "            out_channels = output.shape[1]\n",
    "            num_max = ((output_height + 2 * padding - kernel_size) // stride + 1) * ((output_width + 2 * padding - kernel_size) // stride + 1) * (kernel_size ** 2 - 1) * out_channels  \n",
    "        else:\n",
    "            num_max = 0\n",
    "        return num_max\n",
    "             \n",
    "    def count_conv_operations(self, input, output, output_pooled, conv_layer, pool_layer):\n",
    "        out_channels, in_channels = output.size(1), conv_layer.in_channels\n",
    "        output_height, output_width = output.size(2), output.size(3)\n",
    "        filter_size = conv_layer.kernel_size[0]\n",
    "        stride = conv_layer.stride[0]\n",
    "        padding = conv_layer.padding[0]\n",
    "        num_mults = output_height * output_width * in_channels * filter_size ** 2 * out_channels\n",
    "        num_adds = output_height * output_width * in_channels * filter_size ** 2 * out_channels\n",
    "        num_maxs = self.count_maxpool_operations(output, output_pooled, pool_layer)\n",
    "        if pool_layer:\n",
    "            num_maxs = self.count_maxpool_operations(output, output_pooled, pool_layer)\n",
    "        else:\n",
    "            num_maxs = 0\n",
    "        total_ops = num_mults + num_adds + num_maxs\n",
    "        return num_mults, num_adds, num_maxs, total_ops\n",
    "\n",
    "    def count_operations(self, x):\n",
    "        conv1_out = self.features[0](x)\n",
    "        conv1_out_pooled = self.features[2](F.relu(conv1_out))\n",
    "        conv2_out = self.features[3](conv1_out_pooled)\n",
    "        conv2_out_pooled = self.features[6](F.relu(conv2_out))\n",
    "        \n",
    "        conv3_out = self.features[7](conv2_out_pooled)\n",
    "        conv4_out = self.features[9](F.relu(conv3_out))\n",
    "        conv5_out = self.features[11](F.relu(conv4_out))\n",
    "        conv5_out_pooled = self.features[13](F.relu(conv5_out))\n",
    "        \n",
    "        conv1_ops = self.count_conv_operations(x, conv1_out, conv1_out_pooled, self.features[0], self.features[2])\n",
    "        conv2_ops = self.count_conv_operations(conv1_out_pooled, conv2_out, conv2_out_pooled, self.features[3], self.features[6])\n",
    "        conv3_ops = self.count_conv_operations(conv2_out_pooled, conv3_out, conv3_out, self.features[7], None)\n",
    "        conv4_ops = self.count_conv_operations(conv3_out, conv4_out, conv4_out, self.features[9], None)\n",
    "        conv5_ops = self.count_conv_operations(conv4_out, conv5_out, conv5_out_pooled, self.features[11], self.features[13])\n",
    "        return conv1_ops, conv2_ops, conv3_ops, conv4_ops, conv5_ops\n",
    "\n",
    "\n",
    "    def count_fc_operations(self, input, fc_layer):\n",
    "        in_features = fc_layer.in_features\n",
    "        out_features = fc_layer.out_features    \n",
    "        num_mults = out_features * in_features\n",
    "        num_adds = out_features * in_features\n",
    "        num_maxs = 0\n",
    "        total_ops = num_mults + num_adds\n",
    "        return num_mults, num_adds, num_maxs, total_ops\n",
    "    \n",
    "    def count_total_operations(self, x):\n",
    "        conv1_ops, conv2_ops, conv3_ops, conv4_ops, conv5_ops  = self.count_operations(x)\n",
    "        fc1_ops = self.count_fc_operations(x, self.classifier[0])\n",
    "        fc2_ops = self.count_fc_operations(x, self.classifier[3])\n",
    "        fc3_ops = self.count_fc_operations(x, self.classifier[5])\n",
    "        total_ops = sum(op[3] for op in [conv1_ops, conv2_ops, conv3_ops, conv4_ops, conv5_ops, fc1_ops, fc2_ops, fc3_ops])\n",
    "        return total_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def count_maxpool_operations(self, input, output, pool_layer, out_channels):\n",
    "        kernel_maxpooling = pool_layer.kernel_size\n",
    "        stride = pool_layer.stride\n",
    "        padding = pool_layer.padding\n",
    "        output_height = output.shape[2]\n",
    "        output_width = output.shape[3]\n",
    "        out_channels =  output.shape[1]\n",
    "        num_max = output_height * output_width * (kernel_maxpooling**2 -1) * out_channels\n",
    "        return num_max\n",
    "\n",
    "    def count_conv_operations(self, input, output, output_pooled, conv_layer, pool_layer):\n",
    "        out_channels, in_channels = output.size(1), conv_layer.in_channels\n",
    "        output_height, output_width = output.size(2), output.size(3)\n",
    "        filter_size = conv_layer.kernel_size[0]\n",
    "        stride = conv_layer.stride[0]\n",
    "        padding = conv_layer.padding[0]\n",
    "        num_mults = output_height * output_width * in_channels * filter_size ** 2 * out_channels\n",
    "        num_adds = output_height * output_width * in_channels * filter_size ** 2 * out_channels\n",
    "        num_maxs = self.count_maxpool_operations(output, output_pooled, pool_layer, out_channels)\n",
    "        total_ops = num_mults + num_adds + num_maxs\n",
    "        return num_mults, num_adds, num_maxs, total_ops\n",
    "\n",
    "    def count_operations(self, x):\n",
    "        conv1_out = self.conv1(x)\n",
    "        conv1_out_pooled = self.pool(F.relu(conv1_out))\n",
    "        conv2_out = self.conv2(conv1_out_pooled)\n",
    "        conv2_out_pooled = self.pool(F.relu(conv2_out))\n",
    "        conv1_ops = self.count_conv_operations(x, conv1_out, conv1_out_pooled, self.conv1, self.pool)\n",
    "        conv2_ops = self.count_conv_operations(conv1_out_pooled, conv2_out, conv2_out_pooled, self.conv2, self.pool)\n",
    "        return conv1_ops, conv2_ops        \n",
    "\n",
    "    def count_fc_operations(self, input, fc_layer):\n",
    "        in_features = fc_layer.in_features\n",
    "        out_features = fc_layer.out_features\n",
    "        num_mults = out_features * in_features\n",
    "        num_adds = out_features * in_features\n",
    "        num_maxs = 0    \n",
    "        total_ops = num_mults + num_adds\n",
    "        return num_mults, num_adds, num_maxs, total_ops\n",
    "\n",
    "    def count_total_operations(self, x):\n",
    "        conv1_ops, conv2_ops = self.count_operations(x)\n",
    "        fc1_ops = self.count_fc_operations(x, self.fc1)\n",
    "        fc2_ops = self.count_fc_operations(x, self.fc2)\n",
    "        fc3_ops = self.count_fc_operations(x, self.fc3)\n",
    "        total_ops = sum(op[3] for op in [conv1_ops, conv2_ops, fc1_ops, fc2_ops, fc3_ops])\n",
    "        return total_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:1] Validation Acc: 10.000% | Loss: 2.300% | Ops: 20930688 | Time: 1.119046s | Ops/Sec : 1477619994 \n",
      "[Epoch:2] Validation Acc: 16.920% | Loss: 2.292% | Ops: 20930688 | Time: 1.153205s | Ops/Sec : 11343762501 \n",
      "[Epoch:3] Validation Acc: 18.450% | Loss: 2.226% | Ops: 20930688 | Time: 0.945565s | Ops/Sec : 13834783225 \n",
      "[Epoch:4] Validation Acc: 23.600% | Loss: 2.038% | Ops: 20930688 | Time: 1.150473s | Ops/Sec : 11370695931 \n",
      "[Epoch:5] Validation Acc: 28.420% | Loss: 1.960% | Ops: 20930688 | Time: 1.151441s | Ops/Sec : 11361141658 \n",
      "[Epoch:6] Validation Acc: 31.720% | Loss: 1.861% | Ops: 20930688 | Time: 1.152841s | Ops/Sec : 11347342496 \n",
      "[Epoch:7] Validation Acc: 35.710% | Loss: 1.750% | Ops: 20930688 | Time: 1.141953s | Ops/Sec : 11455528060 \n",
      "[Epoch:8] Validation Acc: 37.880% | Loss: 1.675% | Ops: 20930688 | Time: 1.460503s | Ops/Sec : 8956969775 \n",
      "[Epoch:9] Validation Acc: 40.500% | Loss: 1.626% | Ops: 20930688 | Time: 1.150513s | Ops/Sec : 11370297712 \n",
      "[Epoch:10] Validation Acc: 41.830% | Loss: 1.585% | Ops: 20930688 | Time: 1.319629s | Ops/Sec : 9913148387 \n",
      "[Epoch:11] Validation Acc: 43.490% | Loss: 1.552% | Ops: 20930688 | Time: 1.423885s | Ops/Sec : 9187314296 \n",
      "[Epoch:12] Validation Acc: 44.010% | Loss: 1.526% | Ops: 20930688 | Time: 1.430412s | Ops/Sec : 9145394254 \n",
      "[Epoch:13] Validation Acc: 45.360% | Loss: 1.495% | Ops: 20930688 | Time: 1.261958s | Ops/Sec : 10366177958 \n",
      "[Epoch:14] Validation Acc: 46.090% | Loss: 1.472% | Ops: 20930688 | Time: 1.430428s | Ops/Sec : 9145290600 \n",
      "[Epoch:15] Validation Acc: 46.580% | Loss: 1.457% | Ops: 20930688 | Time: 1.148248s | Ops/Sec : 11392733280 \n",
      "[Epoch:16] Validation Acc: 46.840% | Loss: 1.439% | Ops: 20930688 | Time: 1.262182s | Ops/Sec : 10364337336 \n",
      "[Epoch:17] Validation Acc: 48.070% | Loss: 1.426% | Ops: 20930688 | Time: 1.166488s | Ops/Sec : 11214588949 \n",
      "[Epoch:18] Validation Acc: 49.300% | Loss: 1.396% | Ops: 20930688 | Time: 1.168502s | Ops/Sec : 11195255948 \n",
      "[Epoch:19] Validation Acc: 49.860% | Loss: 1.380% | Ops: 20930688 | Time: 1.403252s | Ops/Sec : 9322403288 \n",
      "[Epoch:20] Validation Acc: 51.200% | Loss: 1.356% | Ops: 20930688 | Time: 1.364452s | Ops/Sec : 9587494854 \n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "net = Net().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)    \n",
    "epoch_nums = 20\n",
    "\n",
    "train(epoch_nums, net, trainloader, testloader, optimizer, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:1] Validation Acc: 38.500% | Loss: 1.573% | Ops: 236091392 | Time: 1.436174s | Ops/Sec : 12986741899 \n",
      "[Epoch:2] Validation Acc: 45.810% | Loss: 1.365% | Ops: 236091392 | Time: 1.335401s | Ops/Sec : 110496463028 \n",
      "[Epoch:3] Validation Acc: 56.780% | Loss: 1.174% | Ops: 236091392 | Time: 1.147871s | Ops/Sec : 128548519612 \n",
      "[Epoch:4] Validation Acc: 61.170% | Loss: 1.088% | Ops: 236091392 | Time: 1.180489s | Ops/Sec : 124996625884 \n",
      "[Epoch:5] Validation Acc: 65.840% | Loss: 0.957% | Ops: 236091392 | Time: 1.163759s | Ops/Sec : 126793563872 \n",
      "[Epoch:6] Validation Acc: 68.770% | Loss: 0.882% | Ops: 236091392 | Time: 1.360822s | Ops/Sec : 108432328570 \n",
      "[Epoch:7] Validation Acc: 68.880% | Loss: 0.890% | Ops: 236091392 | Time: 1.181145s | Ops/Sec : 124927139622 \n",
      "[Epoch:8] Validation Acc: 69.550% | Loss: 0.863% | Ops: 236091392 | Time: 1.164897s | Ops/Sec : 126669700952 \n",
      "[Epoch:9] Validation Acc: 73.790% | Loss: 0.755% | Ops: 236091392 | Time: 1.312687s | Ops/Sec : 112408463688 \n",
      "[Epoch:10] Validation Acc: 73.370% | Loss: 0.782% | Ops: 236091392 | Time: 1.367075s | Ops/Sec : 107936358933 \n",
      "[Epoch:11] Validation Acc: 75.340% | Loss: 0.724% | Ops: 236091392 | Time: 1.154295s | Ops/Sec : 127833061028 \n",
      "[Epoch:12] Validation Acc: 72.440% | Loss: 0.852% | Ops: 236091392 | Time: 1.177514s | Ops/Sec : 125312455054 \n",
      "[Epoch:13] Validation Acc: 74.490% | Loss: 0.740% | Ops: 236091392 | Time: 1.219783s | Ops/Sec : 120969969092 \n",
      "[Epoch:14] Validation Acc: 76.030% | Loss: 0.703% | Ops: 236091392 | Time: 1.379916s | Ops/Sec : 106931980319 \n",
      "[Epoch:15] Validation Acc: 78.100% | Loss: 0.650% | Ops: 236091392 | Time: 1.418651s | Ops/Sec : 104012303788 \n",
      "[Epoch:16] Validation Acc: 76.340% | Loss: 0.699% | Ops: 236091392 | Time: 1.396515s | Ops/Sec : 105660953272 \n",
      "[Epoch:17] Validation Acc: 77.760% | Loss: 0.663% | Ops: 236091392 | Time: 1.253130s | Ops/Sec : 117750851719 \n",
      "[Epoch:18] Validation Acc: 78.580% | Loss: 0.634% | Ops: 236091392 | Time: 1.152269s | Ops/Sec : 128057834987 \n",
      "[Epoch:19] Validation Acc: 79.000% | Loss: 0.630% | Ops: 236091392 | Time: 1.139234s | Ops/Sec : 129523065998 \n",
      "[Epoch:20] Validation Acc: 78.970% | Loss: 0.638% | Ops: 236091392 | Time: 1.172465s | Ops/Sec : 125852011935 \n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "netAlex = AlexNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(netAlex.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "epoch_nums = 20\n",
    "\n",
    "train(epoch_nums, netAlex, trainloader, testloader, optimizer, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:1] Validation Acc: 45.400% | Loss: 1.496% | Ops: 693589248 | Time: 1.166674s | Ops/Sec : 46965599775 \n",
      "[Epoch:2] Validation Acc: 52.410% | Loss: 1.306% | Ops: 693589248 | Time: 1.148276s | Ops/Sec : 377516595129 \n",
      "[Epoch:3] Validation Acc: 59.090% | Loss: 1.138% | Ops: 693589248 | Time: 1.341359s | Ops/Sec : 323174657376 \n",
      "[Epoch:4] Validation Acc: 64.670% | Loss: 0.997% | Ops: 693589248 | Time: 1.140669s | Ops/Sec : 380034366134 \n",
      "[Epoch:5] Validation Acc: 66.960% | Loss: 0.956% | Ops: 693589248 | Time: 1.238172s | Ops/Sec : 350107533092 \n",
      "[Epoch:6] Validation Acc: 69.650% | Loss: 0.874% | Ops: 693589248 | Time: 0.804577s | Ops/Sec : 538784325753 \n",
      "[Epoch:7] Validation Acc: 71.160% | Loss: 0.832% | Ops: 693589248 | Time: 1.426240s | Ops/Sec : 303941233589 \n",
      "[Epoch:8] Validation Acc: 71.430% | Loss: 0.822% | Ops: 693589248 | Time: 1.165017s | Ops/Sec : 372091689359 \n",
      "[Epoch:9] Validation Acc: 73.620% | Loss: 0.754% | Ops: 693589248 | Time: 1.157795s | Ops/Sec : 374412748852 \n",
      "[Epoch:10] Validation Acc: 74.190% | Loss: 0.735% | Ops: 693589248 | Time: 1.375691s | Ops/Sec : 315109442971 \n",
      "[Epoch:11] Validation Acc: 75.660% | Loss: 0.712% | Ops: 693589248 | Time: 1.149309s | Ops/Sec : 377177338913 \n",
      "[Epoch:12] Validation Acc: 74.780% | Loss: 0.714% | Ops: 693589248 | Time: 1.178649s | Ops/Sec : 367788363258 \n",
      "[Epoch:13] Validation Acc: 74.840% | Loss: 0.717% | Ops: 693589248 | Time: 1.409793s | Ops/Sec : 307487258081 \n",
      "[Epoch:14] Validation Acc: 76.920% | Loss: 0.671% | Ops: 693589248 | Time: 1.183206s | Ops/Sec : 366371821585 \n",
      "[Epoch:15] Validation Acc: 76.810% | Loss: 0.654% | Ops: 693589248 | Time: 1.356498s | Ops/Sec : 319567945577 \n",
      "[Epoch:16] Validation Acc: 76.610% | Loss: 0.671% | Ops: 693589248 | Time: 1.426419s | Ops/Sec : 303903284326 \n",
      "[Epoch:17] Validation Acc: 76.840% | Loss: 0.663% | Ops: 693589248 | Time: 1.153311s | Ops/Sec : 375868499192 \n",
      "[Epoch:18] Validation Acc: 78.690% | Loss: 0.616% | Ops: 693589248 | Time: 1.248544s | Ops/Sec : 347198981334 \n",
      "[Epoch:19] Validation Acc: 78.610% | Loss: 0.618% | Ops: 693589248 | Time: 1.165976s | Ops/Sec : 371785826616 \n",
      "[Epoch:20] Validation Acc: 79.320% | Loss: 0.599% | Ops: 693589248 | Time: 1.152991s | Ops/Sec : 375972725841 \n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "netVGG = VGG().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(netVGG.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "epoch_nums = 20\n",
    "\n",
    "train(epoch_nums, netVGG, trainloader, testloader, optimizer, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:1] Validation Acc: 58.140% | Loss: 1.174% | Ops: 1266483200 | Time: 1.217599s | Ops/Sec : 82171716113 \n",
      "[Epoch:2] Validation Acc: 62.710% | Loss: 1.039% | Ops: 1266483200 | Time: 1.151975s | Ops/Sec : 687126214742 \n",
      "[Epoch:3] Validation Acc: 68.810% | Loss: 0.870% | Ops: 1266483200 | Time: 0.882500s | Ops/Sec : 896943086374 \n",
      "[Epoch:4] Validation Acc: 67.330% | Loss: 0.942% | Ops: 1266483200 | Time: 0.999935s | Ops/Sec : 791603524105 \n",
      "[Epoch:5] Validation Acc: 71.870% | Loss: 0.821% | Ops: 1266483200 | Time: 1.143902s | Ops/Sec : 691975467417 \n",
      "[Epoch:6] Validation Acc: 71.730% | Loss: 0.816% | Ops: 1266483200 | Time: 1.209887s | Ops/Sec : 654236547558 \n",
      "[Epoch:7] Validation Acc: 71.020% | Loss: 0.865% | Ops: 1266483200 | Time: 1.354752s | Ops/Sec : 584278128212 \n",
      "[Epoch:8] Validation Acc: 77.350% | Loss: 0.661% | Ops: 1266483200 | Time: 1.176023s | Ops/Sec : 673075540710 \n",
      "[Epoch:9] Validation Acc: 78.180% | Loss: 0.637% | Ops: 1266483200 | Time: 1.167971s | Ops/Sec : 677715655799 \n",
      "[Epoch:10] Validation Acc: 74.380% | Loss: 0.768% | Ops: 1266483200 | Time: 1.376751s | Ops/Sec : 574942041822 \n",
      "[Epoch:11] Validation Acc: 77.460% | Loss: 0.663% | Ops: 1266483200 | Time: 1.063858s | Ops/Sec : 744039125543 \n",
      "[Epoch:12] Validation Acc: 74.910% | Loss: 0.733% | Ops: 1266483200 | Time: 1.191909s | Ops/Sec : 664104614535 \n",
      "[Epoch:13] Validation Acc: 81.070% | Loss: 0.556% | Ops: 1266483200 | Time: 1.219276s | Ops/Sec : 649198275681 \n",
      "[Epoch:14] Validation Acc: 79.730% | Loss: 0.596% | Ops: 1266483200 | Time: 0.813690s | Ops/Sec : 972792571580 \n",
      "[Epoch:15] Validation Acc: 80.420% | Loss: 0.584% | Ops: 1266483200 | Time: 1.174890s | Ops/Sec : 673724598385 \n",
      "[Epoch:16] Validation Acc: 80.470% | Loss: 0.565% | Ops: 1266483200 | Time: 1.349538s | Ops/Sec : 586535605753 \n",
      "[Epoch:17] Validation Acc: 78.930% | Loss: 0.636% | Ops: 1266483200 | Time: 1.155123s | Ops/Sec : 685253576065 \n",
      "[Epoch:18] Validation Acc: 76.820% | Loss: 0.707% | Ops: 1266483200 | Time: 1.205323s | Ops/Sec : 656713734931 \n",
      "[Epoch:19] Validation Acc: 81.010% | Loss: 0.569% | Ops: 1266483200 | Time: 1.389428s | Ops/Sec : 569696347554 \n",
      "[Epoch:20] Validation Acc: 80.920% | Loss: 0.563% | Ops: 1266483200 | Time: 1.162213s | Ops/Sec : 681073179418 \n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "netRes = ResNet18().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(netRes.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "epoch_nums = 20\n",
    "\n",
    "train(epoch_nums, netRes, trainloader, testloader, optimizer, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
