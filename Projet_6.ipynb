{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running on GPU ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train) \n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2) \n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réseau neuronal original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def count_maxpool_operations(self, input, output, pool_layer, out_channels):\n",
    "        kernel_maxpooling = pool_layer.kernel_size\n",
    "        stride = pool_layer.stride\n",
    "        padding = pool_layer.padding\n",
    "        output_height = output.shape[2]\n",
    "        output_width = output.shape[3]\n",
    "        out_channels =  output.shape[1]\n",
    "        num_max = output_height * output_width * (kernel_maxpooling**2 -1) * out_channels\n",
    "        return num_max\n",
    "\n",
    "    def count_conv_operations(self, input, output, output_pooled, conv_layer, pool_layer):\n",
    "        out_channels, in_channels = output.size(1), conv_layer.in_channels\n",
    "        output_height, output_width = output.size(2), output.size(3)\n",
    "        filter_size = conv_layer.kernel_size[0]\n",
    "        stride = conv_layer.stride[0]\n",
    "        padding = conv_layer.padding[0]\n",
    "        num_mults = output_height * output_width * in_channels * filter_size ** 2 * out_channels\n",
    "        num_adds = output_height * output_width * in_channels * filter_size ** 2 * out_channels\n",
    "        num_maxs = self.count_maxpool_operations(output, output_pooled, pool_layer, out_channels)\n",
    "        total_ops = num_mults + num_adds + num_maxs\n",
    "        return num_mults, num_adds, num_maxs, total_ops\n",
    "\n",
    "    def count_operations(self, x):\n",
    "        conv1_out = self.conv1(x)\n",
    "        conv1_out_pooled = self.pool(F.relu(conv1_out))\n",
    "        conv2_out = self.conv2(conv1_out_pooled)\n",
    "        conv2_out_pooled = self.pool(F.relu(conv2_out))\n",
    "        conv1_ops = self.count_conv_operations(x, conv1_out, conv1_out_pooled, self.conv1, self.pool)\n",
    "        conv2_ops = self.count_conv_operations(conv1_out_pooled, conv2_out, conv2_out_pooled, self.conv2, self.pool)\n",
    "        return conv1_ops, conv2_ops        \n",
    "\n",
    "    def count_fc_operations(self, input, fc_layer):\n",
    "        in_features = fc_layer.in_features\n",
    "        out_features = fc_layer.out_features\n",
    "        num_mults = out_features * in_features\n",
    "        num_adds = out_features * in_features\n",
    "        num_maxs = 0    \n",
    "        total_ops = num_mults + num_adds\n",
    "        return num_mults, num_adds, num_maxs, total_ops\n",
    "\n",
    "    def count_total_operations(self, x):\n",
    "        conv1_ops, conv2_ops = self.count_operations(x)\n",
    "        fc1_ops = self.count_fc_operations(x, self.fc1)\n",
    "        fc2_ops = self.count_fc_operations(x, self.fc2)\n",
    "        fc3_ops = self.count_fc_operations(x, self.fc3)\n",
    "        total_ops = sum(op[3] for op in [conv1_ops, conv2_ops, fc1_ops, fc2_ops, fc3_ops])\n",
    "        return total_ops\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dans la partie entraînement du réseau CNN, lister les différentes couches et sous-couches :\n",
    "1. **Deux couches de convolution :**\n",
    "   - `self.conv1`: Première couche de convolution avec 3 canaux d'entrée et 6 canaux de sortie, utilisant un noyau de taille 5x5.\n",
    "   - `self.conv2`: Deuxième couche de convolution avec 6 canaux d'entrée et 16 canaux de sortie, également avec un noyau de taille 5x5.\n",
    "\n",
    "   - **Deux sous-couche non linéaire :** Après chaque opération de convolution, une activation ReLU est appliquée.\n",
    "\n",
    "2. **Deux couches de max pooling :**\n",
    "\n",
    "   - `self.pool`: Utilisé après chaque couche de convolution pour réduire la dimensionnalité de la sortie.\n",
    "\n",
    "3. **Trois couches entièrement connectées :**\n",
    "   - `self.fc1`: Première couche entièrement connectée avec 16*5*5 entrées et 120 sorties.\n",
    "   - `self.fc2`: Deuxième couche entièrement connectée avec 120 entrées et 84 sorties.\n",
    "   - `self.fc3`: Troisième couche entièrement connectée avec 84 entrées et 10 sorties.\n",
    "\n",
    "   - **Trois sous-couche linéaire :** Chaque couche entièrement connectée effectue une transformation linéaire des caractéristiques d'entrée.\n",
    "   - **Deux sous-couche non linéaire :** Après chaque transformation linéaire, une activation ReLU est appliquée, introduisant de la non-linéarité.\n",
    "  \n",
    "4. **Quarte sous-couches Relu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 5, 5])\n",
      "torch.Size([6])\n",
      "torch.Size([16, 6, 5, 5])\n",
      "torch.Size([16])\n",
      "torch.Size([120, 400])\n",
      "torch.Size([120])\n",
      "torch.Size([84, 120])\n",
      "torch.Size([84])\n",
      "torch.Size([10, 84])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "net = Net().to(device)\n",
    "for p in net.parameters(): print(p.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Donner la taille des différents tenseurs de données Xn et de poids Wn le long du calcul. ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Convolutional Layer 1 (conv1):**\n",
    "- X1 = [3, 32, 32]\n",
    "- Poids W1:\n",
    "    - Poids de convolution: torch.Size([6, 3, 5, 5])\n",
    "    - Bias: torch.Size([6])\n",
    "\n",
    "2. **Convolutional Layer 2 (conv2):**\n",
    "    - X2 = [6, 14, 14]\n",
    "- Poids W2:\n",
    "    - Poids de convolution: torch.Size([16, 6, 5, 5])\n",
    "    - Bias: torch.Size([16])\n",
    "\n",
    "3. **Fully Connected Layer 1 (fc1):**\n",
    "- X3: [400]\n",
    "- Poids W3:\n",
    "    - Poids de linéaire: torch.Size([120, 400])\n",
    "    - Bias: torch.Size([120])\n",
    "\n",
    "4. **Fully Connected Layer 2 (fc2):**\n",
    "- X4: [120]\n",
    "- Poids W4:\n",
    "    - Poids de linéaire: torch.Size([84, 120])\n",
    "    - Bias: torch.Size([84])\n",
    "  \n",
    "5. **Fully Connected Layer 3 (fc3):**\n",
    "- X5: [84]\n",
    "- Poids W5:\n",
    "    - Poids de linéaire: torch.Size([10, 84])\n",
    "    - Bias: torch.Size([10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Residual Net ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, inchannel, outchannel, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inchannel, outchannel, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(outchannel)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(outchannel, outchannel, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(outchannel)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or inchannel != outchannel:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(inchannel, outchannel, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(outchannel)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self.make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.fc = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def make_layer(self, block, channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        inchannel = 64 \n",
    "        for stride in strides:\n",
    "            layers.append(block(inchannel, channels, stride))\n",
    "            inchannel = channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer1(out)\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "    def count_conv_operations(self, input, output, output_pooled, stride, padding, kernel_size):\n",
    "        out_channels = output.size(1)\n",
    "        output_height, output_width = output.size(2), output.size(3)\n",
    "        num_mults = output_height * output_width * input.size(1) * kernel_size ** 2 * out_channels\n",
    "        num_adds = output_height * output_width * input.size(1) * kernel_size ** 2 * out_channels\n",
    "        num_maxs = self.count_maxpool_operations(output, output_pooled)\n",
    "        total_ops = num_mults + num_adds + num_maxs\n",
    "        return num_mults, num_adds, num_maxs, total_ops\n",
    "\n",
    "    def count_maxpool_operations(self, output, output_pooled):\n",
    "        kernel_size = 2 \n",
    "        stride = 2 \n",
    "        padding = 0\n",
    "        output_height = output.shape[2]\n",
    "        output_width = output.shape[3]\n",
    "        out_channels = output.shape[1]\n",
    "        num_max = ((output_height + 2 * padding - kernel_size) // stride + 1) * ((output_width + 2 * padding - kernel_size) // stride + 1) * (kernel_size ** 2 - 1) * out_channels\n",
    "        return num_max\n",
    "\n",
    "    def count_fc_operations(self, input, fc_layer):\n",
    "        in_features = fc_layer.in_features\n",
    "        out_features = fc_layer.out_features    \n",
    "        num_mults = out_features * in_features\n",
    "        num_adds = out_features * in_features\n",
    "        num_maxs = 0\n",
    "        total_ops = num_mults + num_adds\n",
    "        return num_mults, num_adds, num_maxs, total_ops\n",
    "\n",
    "    def count_total_operations(self, x):\n",
    "        conv1_out = self.conv1(x)\n",
    "        conv1_out_pooled = self.relu(self.bn1(conv1_out))\n",
    "        conv1_ops = self.count_conv_operations(x, conv1_out, conv1_out_pooled, self.conv1, None, kernel_size=3)\n",
    "\n",
    "        layer1_out = self.layer1(conv1_out_pooled)\n",
    "        layer1_out_pooled = F.avg_pool2d(layer1_out, 8)\n",
    "        layer1_ops = self.count_conv_operations(conv1_out_pooled, layer1_out, layer1_out_pooled, stride=1, padding=1, kernel_size=3)\n",
    "\n",
    "        fc_ops = self.count_fc_operations(x, self.fc)\n",
    "\n",
    "        total_ops = sum(op[3] for op in [conv1_ops, layer1_ops, fc_ops])\n",
    "        return total_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet18():\n",
    "    return ResNet(ResidualBlock, [2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Le nombre de couches et sous-couches de Residual Net:**\n",
    "- 9 couches de convolution: Dans ce cas **[2, 2]**, la fonction ResNet18 renvoie un modèle ResNet comprenant des blocs résiduels de type ResidualBlock et deux étapes, chaque étape ayant 2 blocs résiduels. Par conséquent, il y a un total de 4 blocs résiduels. Chaque bloc résiduel contient 2 couches de convolution. De plus, dans le modèle ResNet, il y a une couche de convolution initiale `self.conv1`. Ainsi, 1 + 4 * 2 = 9 couches de convolution.\n",
    "  \n",
    "- 9 couches de Batch Normalization: \n",
    "Chaque bloc résiduel contient deux couches de Batch Normalization, donc avec quatre blocs résiduels, il y a au total 2 * 4 = 8 couches de Batch Normalization. Ajoutant la couche de Batch Normalization utilisée après la première couche de convolution, cela donne un total de 8 + 1 = 9 couches de Batch Normalization\n",
    "\n",
    "- 8 couches ReLU: Chaque bloc résiduel contient 2 sous-couches ReLU, une après chaque opération de convolution. Comme il y a 4 blocs résiduels dans ResNet18 **[2, 2]**, cela fait un total de 4 * 2 = 8 sous-couches ReLU.\n",
    "  \n",
    "- 1 couche entièrement connectée\n",
    "- 1 sous-couche de Average Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 3, 3])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 64, 3, 3])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 64, 3, 3])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 64, 3, 3])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 64, 3, 3])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([10, 1024])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "netRes = ResNet18().to(device)\n",
    "for p in netRes.parameters(): print(p.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. identification la taille des différents tenseurs de données Xn et de poids Wn le long du calcul:**\n",
    "\n",
    "**Données d'entrée :**\n",
    "- X0: [3, 32, 32]\n",
    "\n",
    "**Couche de convolution 1 (conv1) :**\n",
    "- Données de sortie (X1) : Taille [64, 32, 32]\n",
    "- Poids du filtre (W1) : Taille **[64, 3, 3, 3]**\n",
    "- Biais : Taille [64]\n",
    "\n",
    "**Couche de normalisation par lots 1 :**\n",
    "- Données de sortie (X2) : Taille [64, 32, 32]\n",
    "\n",
    "**Bloc résiduel 1 :**\n",
    "- Données de sortie (X3) : Taille [64, 32, 32]\n",
    "- Poids du premier filtre (W2) : Taille **[64, 64, 3, 3]**\n",
    "- Biais du premier filtre: Taille [64]\n",
    "- Poids du deuxième filtre (W3) : Taille **[64, 64, 3, 3]**\n",
    "- Biais du deuxième filtre: Taille [64]\n",
    "\n",
    "**Bloc résiduel 2 :**\n",
    "- Données de sortie (X4) : Taille [64, 32, 32]\n",
    "- Poids du premier filtre (W4) : Taille **[64, 64, 3, 3]**\n",
    "- Biais du premier filtre: Taille [64]\n",
    "- Poids du deuxième filtre (W5) : Taille **[64, 64, 3, 3]**\n",
    "- Biais du deuxième filtre: Taille [64]\n",
    "\n",
    "**Couche de pooling moyenne :**\n",
    "- Données de sortie (X5) : Taille [64, 4, 4]\n",
    "\n",
    "**Couche d'aplatissement :**\n",
    "- Données de sortie (X6) : Taille [1024]\n",
    "\n",
    "**Couche entièrement connectée (fc) :**\n",
    "- Données de sortie (X7) : Taille [10]\n",
    "- Poids du filtre (W7) : Taille **[10, 1024]**\n",
    "- Biais: Taille [10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. identification de toutes les fonctions successives (les “Fn”) avec leurs types:**\n",
    "\n",
    "**Couche de convolution 1 (conv1) :**\n",
    "- Type de fonction : Convolution nn.Conv2d\n",
    "- Fonction d'activation : nn.ReLU\n",
    "- Fonction de normalisation : nn.BatchNorm2d\n",
    "\n",
    "**Blocs résiduels (layer1) :**\n",
    "- Type de fonction : Bloc résiduel personnalisé ResidualBlock\n",
    "- Fonction de convolution : nn.Conv2d\n",
    "- Fonction d'activation : nn.ReLU\n",
    "- Fonction de normalisation : nn.BatchNorm2d\n",
    "\n",
    "**Couche entièrement connectée (fc) :**\n",
    "- Type de fonction : nn.Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model AlexNet ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "  \n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, stride=1, padding=2)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=48, kernel_size=3, padding=1)\n",
    "        self.batchnorm = nn.BatchNorm2d(48)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n",
    "        self.conv3 = nn.Conv2d(in_channels=48, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=64, out_channels=48, kernel_size=3, padding=1)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=3*3*48, out_features=128)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.maxpool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = self.maxpool3(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def count_maxpool_operations(self, input, output, pool_layer):\n",
    "        if pool_layer:\n",
    "            kernel_size = pool_layer.kernel_size\n",
    "            stride = pool_layer.stride\n",
    "            padding = pool_layer.padding\n",
    "            output_height = output.shape[2]\n",
    "            output_width = output.shape[3]\n",
    "            out_channels = output.shape[1]\n",
    "            num_max = ((output_height + 2 * padding - kernel_size) // stride + 1) * ((output_width + 2 * padding - kernel_size) // stride + 1) * (kernel_size ** 2 - 1) * out_channels  \n",
    "        else:\n",
    "            num_max = 0\n",
    "        return num_max\n",
    "\n",
    "    def count_conv_operations(self, input, output, output_pooled, conv_layer, pool_layer):\n",
    "        out_channels, in_channels = output.size(1), conv_layer.in_channels\n",
    "        output_height, output_width = output.size(2), output.size(3)\n",
    "        filter_size = conv_layer.kernel_size[0]\n",
    "        stride = conv_layer.stride[0]\n",
    "        padding = conv_layer.padding[0]\n",
    "        num_mults = output_height * output_width * in_channels * filter_size ** 2 * out_channels\n",
    "        num_adds = output_height * output_width * in_channels * filter_size ** 2 * out_channels\n",
    "        num_maxs = self.count_maxpool_operations(output, output_pooled, pool_layer)\n",
    "        if pool_layer:\n",
    "            num_maxs = self.count_maxpool_operations(output, output_pooled, pool_layer)\n",
    "        else:\n",
    "            num_maxs = 0\n",
    "        total_ops = num_mults + num_adds + num_maxs\n",
    "        return num_mults, num_adds, num_maxs, total_ops\n",
    "\n",
    "    def count_operations(self, x):\n",
    "        conv1_out = F.relu(self.conv1(x))\n",
    "        conv1_out_pooled = self.maxpool1(conv1_out)\n",
    "        conv2_out = F.relu(self.conv2(conv1_out_pooled))\n",
    "        conv2_out_norm = self.batchnorm(conv2_out)\n",
    "        conv2_out_pooled = self.maxpool2(conv2_out_norm)\n",
    "        conv3_out = F.relu(self.conv3(conv2_out_pooled))\n",
    "        conv4_out = F.relu(self.conv4(conv3_out))\n",
    "        conv5_out = F.relu(self.conv5(conv4_out))\n",
    "        conv5_out_pooled = self.maxpool3(conv5_out)\n",
    "        \n",
    "        conv1_ops = self.count_conv_operations(x, conv1_out, conv1_out_pooled, self.conv1, self.maxpool1)\n",
    "        conv2_ops = self.count_conv_operations(conv1_out_pooled, conv2_out, conv2_out_pooled, self.conv2, self.maxpool2)\n",
    "        conv3_ops = self.count_conv_operations(conv2_out_pooled, conv3_out, conv3_out, self.conv3, None)\n",
    "        conv4_ops = self.count_conv_operations(conv3_out, conv4_out, conv4_out, self.conv4, None)\n",
    "        conv5_ops = self.count_conv_operations(conv4_out, conv5_out, conv5_out_pooled, self.conv5, self.maxpool3)\n",
    "        \n",
    "        return conv1_ops, conv2_ops, conv3_ops, conv4_ops, conv5_ops\n",
    "        \n",
    "    def count_fc_operations(self, input, fc_layer):\n",
    "        in_features = fc_layer.in_features\n",
    "        out_features = fc_layer.out_features\n",
    "        num_mults = out_features * in_features\n",
    "        num_adds = out_features * in_features\n",
    "        num_maxs = 0\n",
    "        total_ops = num_mults + num_adds\n",
    "        return num_mults, num_adds, num_maxs, total_ops\n",
    "    \n",
    "    def count_total_operations(self, x):\n",
    "        conv1_ops, conv2_ops, conv3_ops, conv4_ops, conv5_ops = self.count_operations(x)\n",
    "        fc1_ops = self.count_fc_operations(x, self.fc1)\n",
    "        fc2_ops = self.count_fc_operations(x, self.fc2)\n",
    "        fc3_ops = self.count_fc_operations(x, self.fc3)\n",
    "        total_ops = sum(op[3] for op in [conv1_ops, conv2_ops, conv3_ops, conv4_ops, conv5_ops, fc1_ops, fc2_ops, fc3_ops])\n",
    "        return total_ops\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Le nombre de couches et sous-couches de AlexNet:**\n",
    "\n",
    "1. **Cinq couches de convolution :**\n",
    "   - `self.conv1`: Première couche de convolution avec 3 canaux d'entrée et 16 canaux de sortie, utilisant un noyau de taille 5x5.\n",
    "   - `self.conv2`: Deuxième couche de convolution avec 16 canaux d'entrée et 48 canaux de sortie, utilisant un noyau de taille 3x3.\n",
    "   - `self.conv3`: Troisième couche de convolution avec 48 canaux d'entrée et 64 canaux de sortie, utilisant un noyau de taille 3x3.\n",
    "   - `self.conv4`: Quatrième couche de convolution avec 64 canaux d'entrée et 64 canaux de sortie, utilisant un noyau de taille 3x3.\n",
    "   - `self.conv5`: Cinquième couche de convolution avec 64 canaux d'entrée et 48 canaux de sortie, utilisant un noyau de taille 3x3.\n",
    "\n",
    "2. **Trois couches de max pooling :**\n",
    "   - Trois Max pooling avec un noyau de taille 3x3 et un stride de 2.\n",
    "\n",
    "3. **Trois couches entièrement connectées :**\n",
    "   - `self.fc1`: Première couche entièrement connectée avec 432 entrées et 128 sorties.\n",
    "   - `self.fc2`: Deuxième couche entièrement connectée avec 128 entrées et 128 sorties.\n",
    "   - `self.fc3`: Troisième couche entièrement connectée avec 128 entrées et 10 sorties.\n",
    "\n",
    "4. **Sept sous-couches ReLU :**\n",
    "   - Utilisées après chaque opération de convolution et les deux premières couches linéaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 5, 5])\n",
      "torch.Size([16])\n",
      "torch.Size([48, 16, 3, 3])\n",
      "torch.Size([48])\n",
      "torch.Size([48])\n",
      "torch.Size([48])\n",
      "torch.Size([64, 48, 3, 3])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 64, 3, 3])\n",
      "torch.Size([64])\n",
      "torch.Size([48, 64, 3, 3])\n",
      "torch.Size([48])\n",
      "torch.Size([128, 432])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 128])\n",
      "torch.Size([128])\n",
      "torch.Size([10, 128])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "netAlex = AlexNet().to(device)\n",
    "for p in netAlex.parameters(): print(p.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. identification la taille des différents tenseurs de données Xn et de poids Wn le long du calcul:**\n",
    "\n",
    "1. **Couche de convolution 1 (conv1) :**\n",
    "   - X1: [3, 32, 32]\n",
    "   - W1: [16, 3, 5, 5]\n",
    "   - Bias: [16]\n",
    "\n",
    "2. **Max pooling 1 (maxpool1) :**\n",
    "   - X2: [16, 14, 14]\n",
    "\n",
    "3. **Couche de convolution 2 (conv2) :**\n",
    "   - X3: [16, 14, 14]\n",
    "   - W2: [48, 16, 3, 3]\n",
    "   - Bias: [48]\n",
    "\n",
    "4. **Couche de normalisation par lots (batchnorm) :**\n",
    "   - X4: [48, 14, 14]\n",
    "\n",
    "5. **Max pooling 2 (maxpool2) :**\n",
    "   - X5: [48, 6, 6]\n",
    "\n",
    "6. **Couche de convolution 3 (conv3) :**\n",
    "   - X6: [48, 6, 6]\n",
    "   - W3: [64, 48, 3, 3]\n",
    "   - Bias: [64]\n",
    "\n",
    "7. **Couche de convolution 4 (conv4) :**\n",
    "   - X7: [64, 6, 6]\n",
    "   - W4: [64, 64, 3, 3]\n",
    "   - Bias: [64]\n",
    "\n",
    "8. **Couche de convolution 5 (conv5) :**\n",
    "   - X8: [64, 6, 6]\n",
    "   - W5: [48, 64, 3, 3]\n",
    "   - Bias: [48]\n",
    "\n",
    "9. **Max pooling 3 (maxpool3) :**\n",
    "   - X9: [48, 2, 2]\n",
    "\n",
    "10. **Couche entièrement connectée 1 (fc1) :**\n",
    "   - X10: [1, 432]\n",
    "   - W6: [128, 432]\n",
    "   - Bias: [128]\n",
    "\n",
    "11. **Couche entièrement connectée 2 (fc2) :**\n",
    "   - X11: [1, 128]\n",
    "   - W7: [128, 128]\n",
    "   - Bias: [128]\n",
    "\n",
    "12. **Couche entièrement connectée 3 (fc3) :**\n",
    "   - X12: [1, 128]\n",
    "   - W8: [10, 128]\n",
    "   - Bias: [10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. identification de toutes les fonctions successives (les “Fn”) avec leurs types:**\n",
    "\n",
    "**Couche de convolution 1 (conv1) :**\n",
    "- Type de fonction : Convolution - `nn.Conv2d`\n",
    "- Fonction d'activation : ReLU - `nn.ReLU`\n",
    "- Fonction de normalisation : `nn.BatchNorm2d`\n",
    "\n",
    "**Couche de pooling maximal 1 (maxpool1) :**\n",
    "- Type de fonction : Pooling maximal - `nn.MaxPool2d`\n",
    "\n",
    "**Couche de convolution 2 (conv2) :**\n",
    "- Type de fonction : Convolution - `nn.Conv2d`\n",
    "- Fonction d'activation : ReLU - `nn.ReLU`\n",
    "\n",
    "**Couche de normalisation par lots (batchnorm) :**\n",
    "- Type de fonction : `nn.BatchNorm2d`\n",
    "\n",
    "**Couche de pooling maximal 2 (maxpool2) :**\n",
    "- Type de fonction : Pooling maximal - `nn.MaxPool2d`\n",
    "\n",
    "**Couche de convolution 3 (conv3) :**\n",
    "- Type de fonction : Convolution - `nn.Conv2d`\n",
    "- Fonction d'activation : ReLU - `nn.ReLU`\n",
    "\n",
    "**Couche de convolution 4 (conv4) :**\n",
    "- Type de fonction : Convolution - `nn.Conv2d`\n",
    "- Fonction d'activation : ReLU - `nn.ReLU`\n",
    "\n",
    "**Couche de convolution 5 (conv5) :**\n",
    "- Type de fonction : Convolution - `nn.Conv2d`\n",
    "- Fonction d'activation : ReLU - `nn.ReLU`\n",
    "\n",
    "**Couche de pooling maximal 3 (maxpool3) :**\n",
    "- Type de fonction : Pooling maximal - `nn.MaxPool2d`\n",
    "\n",
    "**Couche entièrement connectée 1 (fc1) :**\n",
    "- Type de fonction : Entièrement connectée - `nn.Linear`\n",
    "- Fonction d'activation : ReLU - `nn.ReLU`\n",
    "\n",
    "**Couche de régularisation (dropout) :**\n",
    "- Type de fonction : Régularisation par abandon - `nn.Dropout`\n",
    "\n",
    "**Couche entièrement connectée 2 (fc2) :**\n",
    "- Type de fonction : Entièrement connectée - `nn.Linear`\n",
    "- Fonction d'activation : ReLU - `nn.ReLU`\n",
    "\n",
    "**Couche entièrement connectée 3 (fc3) :**\n",
    "- Type de fonction : Entièrement connectée - `nn.Linear`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model VGG ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def count_maxpool_operations(self, input, output, pool_layer, out_channels):\n",
    "        kernel_maxpooling = pool_layer.kernel_size\n",
    "        stride = pool_layer.stride\n",
    "        padding = pool_layer.padding\n",
    "        output_height = output.shape[2]\n",
    "        output_width = output.shape[3]\n",
    "        out_channels =  output.shape[1]\n",
    "        num_max = output_height * output_width * (kernel_maxpooling**2 -1) * out_channels\n",
    "        return num_max\n",
    "\n",
    "    def count_conv_operations(self, input, output, output_pooled, conv_layer, pool_layer):\n",
    "        out_channels, in_channels = output.size(1), conv_layer.in_channels\n",
    "        output_height, output_width = output.size(2), output.size(3)\n",
    "        filter_size = conv_layer.kernel_size[0]\n",
    "        stride = conv_layer.stride[0]\n",
    "        padding = conv_layer.padding[0]\n",
    "        num_mults = output_height * output_width * in_channels * filter_size ** 2 * out_channels\n",
    "        num_adds = output_height * output_width * in_channels * filter_size ** 2 * out_channels\n",
    "        num_maxs = self.count_maxpool_operations(output, output_pooled, pool_layer, out_channels)\n",
    "        total_ops = num_mults + num_adds + num_maxs\n",
    "        return num_mults, num_adds, num_maxs, total_ops\n",
    "\n",
    "    def count_operations(self, x):\n",
    "        conv1_out = self.conv1(x)\n",
    "        conv1_out_pooled = self.pool(F.relu(conv1_out))\n",
    "        conv2_out = self.conv2(conv1_out_pooled)\n",
    "        conv2_out_pooled = self.pool(F.relu(conv2_out))\n",
    "        conv1_ops = self.count_conv_operations(x, conv1_out, conv1_out_pooled, self.conv1, self.pool)\n",
    "        conv2_ops = self.count_conv_operations(conv1_out_pooled, conv2_out, conv2_out_pooled, self.conv2, self.pool)\n",
    "        return conv1_ops, conv2_ops        \n",
    "\n",
    "    def count_fc_operations(self, input, fc_layer):\n",
    "        in_features = fc_layer.in_features\n",
    "        out_features = fc_layer.out_features\n",
    "        num_mults = out_features * in_features\n",
    "        num_adds = out_features * in_features\n",
    "        num_maxs = 0    \n",
    "        total_ops = num_mults + num_adds\n",
    "        return num_mults, num_adds, num_maxs, total_ops\n",
    "\n",
    "    def count_total_operations(self, x):\n",
    "        conv1_ops, conv2_ops = self.count_operations(x)\n",
    "        fc1_ops = self.count_fc_operations(x, self.fc1)\n",
    "        fc2_ops = self.count_fc_operations(x, self.fc2)\n",
    "        fc3_ops = self.count_fc_operations(x, self.fc3)\n",
    "        total_ops = sum(op[3] for op in [conv1_ops, conv2_ops, fc1_ops, fc2_ops, fc3_ops])\n",
    "        return total_ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Le nombre de couches et sous-couches de VGG:**\n",
    "\n",
    "1. **Deux couches de convolution :**\n",
    "   - `self.conv1`: Première couche de convolution avec 3 canaux d'entrée et 64 canaux de sortie, utilisant un noyau de taille 3x3.\n",
    "   - `self.conv2`: Deuxième couche de convolution avec 64 canaux d'entrée et 128 canaux de sortie, utilisant un noyau de taille 3x3.\n",
    "\n",
    "2. **Deux couches de max pooling :**\n",
    "   - `self.pool`: Utilisé après chaque couche de convolution pour réduire la dimensionnalité de la sortie.\n",
    "\n",
    "3. **Trois couches entièrement connectées :**\n",
    "   - `self.fc1`: Première couche entièrement connectée avec 8192 entrées (128 * 8 * 8) et 120 sorties.\n",
    "   - `self.fc2`: Deuxième couche entièrement connectée avec 120 entrées et 84 sorties.\n",
    "   - `self.fc3`: Troisième couche entièrement connectée avec 84 entrées et 10 sorties.\n",
    "  \n",
    "4. **Quarte sous-couches Relu**\n",
    "   - Utilisées après chaque opération de convolution et les deux premières couches linéaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 3, 3])\n",
      "torch.Size([64])\n",
      "torch.Size([128, 64, 3, 3])\n",
      "torch.Size([128])\n",
      "torch.Size([120, 8192])\n",
      "torch.Size([120])\n",
      "torch.Size([84, 120])\n",
      "torch.Size([84])\n",
      "torch.Size([10, 84])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "netVGG = VGG().to(device)\n",
    "for p in netVGG.parameters(): print(p.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. identification la taille des différents tenseurs de données Xn et de poids Wn le long du calcul:**\n",
    "\n",
    "1. **Couche de convolution 1 (conv1) :**\n",
    "   - Données d'entrée (X0) : [3, 32, 32]\n",
    "   - Poids du filtre (W1) : [64, 3, 3, 3]\n",
    "   - Biais (b1) : [64]\n",
    "   - Données de sortie (X1) : [64, 32, 32]\n",
    "\n",
    "2. **Couche de max pooling 1 (pool1) :**\n",
    "   - Données de sortie (X2) : [64, 16, 16]\n",
    "\n",
    "3. **Couche de convolution 2 (conv2) :**\n",
    "   - Poids du filtre (W2) : [128, 64, 3, 3]\n",
    "   - Biais (b2) : [128]\n",
    "   - Données de sortie (X3) : [128, 16, 16]\n",
    "\n",
    "4. **Couche de max pooling 2 (pool2) :**\n",
    "   - Données de sortie (X4) : [128, 8, 8]\n",
    "\n",
    "5. **Aplatissement (flatten) :**\n",
    "   - Données de sortie (X5) : [8192]\n",
    "\n",
    "6. **Couche entièrement connectée 1 (fc1) :**\n",
    "   - Poids du filtre (W3) : [120, 8192]\n",
    "   - Biais (b3) : [120]\n",
    "   - Données de sortie (X6) : [120]\n",
    "\n",
    "7. **Couche entièrement connectée 2 (fc2) :**\n",
    "   - Poids du filtre (W4) : [84, 120]\n",
    "   - Biais (b4) : [84]\n",
    "   - Données de sortie (X7) : [84]\n",
    "\n",
    "8. **Couche entièrement connectée 3 (fc3) :**\n",
    "   - Poids du filtre (W5) : [10, 84]\n",
    "   - Biais (b5) : [10]\n",
    "   - Données de sortie (X8) : [10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. identification de toutes les fonctions successives (les “Fn”) avec leurs types:**\n",
    "\n",
    "1. **Couche de convolution 1 (conv1) :**\n",
    "   - Type de fonction : Convolution : nn.Conv2d\n",
    "   - Fonction d'activation : F.relu\n",
    "\n",
    "2. **Couche de max pooling 1 (pool1) :**\n",
    "   - Type de fonction : nn.MaxPool2d\n",
    "\n",
    "3. **Couche de convolution 2 (conv2) :**\n",
    "   - Type de fonction : Convolution : nn.Conv2d\n",
    "   - Fonction d'activation : F.relu\n",
    "\n",
    "4. **Couche de max pooling 2 (pool2) :**\n",
    "   - Type de fonction : nn.MaxPool2d\n",
    "\n",
    "5. **Aplatissement (flatten) :**\n",
    "   - Type de fonction : torch.flatten\n",
    "\n",
    "6. **Couche entièrement connectée 1 (fc1) :**\n",
    "   - Type de fonction : Couche linéaire (nn.Linear)\n",
    "   - Fonction d'activation : F.relu\n",
    "\n",
    "7. **Couche entièrement connectée 2 (fc2) :**\n",
    "   - Type de fonction : Couche linéaire (nn.Linear)\n",
    "   - Fonction d'activation : F.relu\n",
    "\n",
    "8. **Couche entièrement connectée 3 (fc3) :**\n",
    "   - Type de fonction : Couche linéaire (nn.Linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction **test_model** évalue les performances d'un modèle de réseau de neurones sur un ensemble de données de test, en calculant la précision, l'erreur, le nombre total d'opérations effectuées pour une image, le temps d'évaluation et le taux d'opérations par seconde sur la durée de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(net, testloader, criterion, device):\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total_samples = 0 \n",
    "    test_loss = 0.0\n",
    "    total_test_ops = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total_test_ops += net.count_total_operations(images)\n",
    "\n",
    "        test_evaluate_time = time.time() - start_time\n",
    "\n",
    "    test_accuracy = 100. * correct / total_samples\n",
    "    test_loss /= len(testloader)\n",
    "    test_ops_per_second = total_test_ops / test_evaluate_time\n",
    "\n",
    "    return test_accuracy, test_loss, total_test_ops, test_evaluate_time, test_ops_per_second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction **train_model** entraîne un modèle de réseau de neurones sur plusieurs époques, évalue les performances après chaque époque et avant la première, affiche l'évolution de l'erreur ou de la précision, calcule le nombre total d'opérations pour chaque époque, y compris les passes avant et arrière pour l'entraînement et une seule passe avant pour les tests, et calcule le nombre total d'opérations flottantes par seconde pendant l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epoch_nums, net, trainloader, testloader, optimizer, criterion, device):\n",
    "    total_time = 0.0\n",
    "    total_ops = 0\n",
    "    epoch_train_ops = 0\n",
    "    \n",
    "    for epoch in range(epoch_nums):\n",
    "        net.train()\n",
    "        sum_loss = 0.0\n",
    "        correct = 0.0\n",
    "        total = 0.0\n",
    "        iteration_num = 0\n",
    "        total_train_ops = 0\n",
    "        epoch_train_time = 0.0\n",
    "\n",
    "        if epoch > 0: \n",
    "            for i, data in enumerate(trainloader, 0):\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "    \n",
    "                if epoch == 0 and i == 0: \n",
    "                    batch_train_ops = net.count_total_operations(inputs)\n",
    "                    batch_train_ops = inputs.shape[0] * batch_train_ops\n",
    "                    epoch_train_ops += batch_train_ops\n",
    "    \n",
    "                if torch.cuda.is_available(): \n",
    "                    torch.cuda.synchronize()\n",
    "                batch_train_start = time.time()\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                sum_loss += loss.item()\n",
    "    \n",
    "                if torch.cuda.is_available(): \n",
    "                    torch.cuda.synchronize()\n",
    "                batch_train_time = time.time() - batch_train_start\n",
    "                epoch_train_time += batch_train_time\n",
    "    \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                iteration_num += 1\n",
    "    \n",
    "                batch_train_ops = net.count_total_operations(inputs)\n",
    "                batch_train_ops = inputs.shape[0] * batch_train_ops\n",
    "                total_train_ops += batch_train_ops\n",
    "    \n",
    "            epoch_train_ops += total_train_ops\n",
    "\n",
    "        test_accuracy, test_loss, total_test_ops, test_evaluate_time, test_ops_per_second = test_model(net, testloader, criterion, device)\n",
    "        total_time += test_evaluate_time + epoch_train_time\n",
    "        train_ops_per_second = epoch_train_ops / total_time\n",
    "        total_ops += epoch_train_ops + total_test_ops\n",
    "        \n",
    "        print('[Epoch:%d] Test Acc: %.3f%% | Loss: %.3f%% | Train Ops: %d | Test Ops: %d | Time: %.6fs | Train Ops/Sec : %d ' % (\n",
    "            epoch, \n",
    "            test_accuracy,\n",
    "            test_loss,\n",
    "            epoch_train_ops,\n",
    "            total_test_ops,\n",
    "            test_evaluate_time,\n",
    "            train_ops_per_second\n",
    "        ))\n",
    "\n",
    "    total_params = sum(p.numel() for p in net.parameters())\n",
    "    print('Time elapsed: %.3fs | Total Ops: %d | Total Parameters : %d ' % (\n",
    "        total_time,\n",
    "        total_ops,\n",
    "        total_params,\n",
    "    ))\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:0] Test Acc: 10.000% | Loss: 2.305% | Train Ops: 0 | Test Ops: 103345272 | Time: 1.215587s | Train Ops/Sec : 0 \n",
      "[Epoch:1] Test Acc: 10.000% | Loss: 2.301% | Train Ops: 65408400000 | Test Ops: 103345272 | Time: 1.150398s | Train Ops/Sec : 20918008436 \n",
      "[Epoch:2] Test Acc: 11.540% | Loss: 2.296% | Train Ops: 130816800000 | Test Ops: 103345272 | Time: 1.206088s | Train Ops/Sec : 25858797015 \n",
      "[Epoch:3] Test Acc: 18.450% | Loss: 2.265% | Train Ops: 196225200000 | Test Ops: 103345272 | Time: 1.133450s | Train Ops/Sec : 28213423627 \n",
      "[Epoch:4] Test Acc: 24.820% | Loss: 2.069% | Train Ops: 261633600000 | Test Ops: 103345272 | Time: 1.131387s | Train Ops/Sec : 29712809298 \n",
      "[Epoch:5] Test Acc: 29.550% | Loss: 1.962% | Train Ops: 327042000000 | Test Ops: 103345272 | Time: 1.130169s | Train Ops/Sec : 30509397783 \n",
      "[Epoch:6] Test Acc: 31.270% | Loss: 1.872% | Train Ops: 392450400000 | Test Ops: 103345272 | Time: 1.123405s | Train Ops/Sec : 31136114028 \n",
      "[Epoch:7] Test Acc: 34.600% | Loss: 1.805% | Train Ops: 457858800000 | Test Ops: 103345272 | Time: 1.125283s | Train Ops/Sec : 31608494069 \n",
      "[Epoch:8] Test Acc: 36.260% | Loss: 1.744% | Train Ops: 523267200000 | Test Ops: 103345272 | Time: 1.354189s | Train Ops/Sec : 31608078195 \n",
      "[Epoch:9] Test Acc: 37.980% | Loss: 1.688% | Train Ops: 588675600000 | Test Ops: 103345272 | Time: 1.234318s | Train Ops/Sec : 31799736355 \n",
      "Time elapsed: 18.512s | Total Ops: 2944411452720 | Total Parameters : 62006 \n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "net = Net().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)    \n",
    "epoch_nums = 10\n",
    "\n",
    "train_model(epoch_nums, net, trainloader, testloader, optimizer, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:0] Test Acc: 10.000% | Loss: 2.304% | Train Ops: 0 | Test Ops: 1165701248 | Time: 1.412438s | Train Ops/Sec : 0 \n",
      "[Epoch:1] Test Acc: 34.080% | Loss: 1.705% | Train Ops: 737785600000 | Test Ops: 1165701248 | Time: 1.160699s | Train Ops/Sec : 173848756845 \n",
      "[Epoch:2] Test Acc: 42.820% | Loss: 1.490% | Train Ops: 1475571200000 | Test Ops: 1165701248 | Time: 1.125994s | Train Ops/Sec : 209727245765 \n",
      "[Epoch:3] Test Acc: 53.170% | Loss: 1.284% | Train Ops: 2213356800000 | Test Ops: 1165701248 | Time: 1.218655s | Train Ops/Sec : 220868966199 \n",
      "[Epoch:4] Test Acc: 60.410% | Loss: 1.107% | Train Ops: 2951142400000 | Test Ops: 1165701248 | Time: 1.131044s | Train Ops/Sec : 230384591550 \n",
      "[Epoch:5] Test Acc: 63.680% | Loss: 1.037% | Train Ops: 3688928000000 | Test Ops: 1165701248 | Time: 1.149699s | Train Ops/Sec : 235204726598 \n",
      "[Epoch:6] Test Acc: 67.230% | Loss: 0.960% | Train Ops: 4426713600000 | Test Ops: 1165701248 | Time: 1.243838s | Train Ops/Sec : 237843428925 \n",
      "[Epoch:7] Test Acc: 67.950% | Loss: 0.892% | Train Ops: 5164499200000 | Test Ops: 1165701248 | Time: 1.116774s | Train Ops/Sec : 240775626622 \n",
      "[Epoch:8] Test Acc: 70.230% | Loss: 0.868% | Train Ops: 5902284800000 | Test Ops: 1165701248 | Time: 1.209388s | Train Ops/Sec : 242156547779 \n",
      "[Epoch:9] Test Acc: 73.130% | Loss: 0.789% | Train Ops: 6640070400000 | Test Ops: 1165701248 | Time: 1.123937s | Train Ops/Sec : 243999478883 \n",
      "Time elapsed: 27.213s | Total Ops: 33212009012480 | Total Parameters : 173834 \n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "netAlex = AlexNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(netAlex.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "epoch_nums = 10\n",
    "\n",
    "train_model(epoch_nums, netAlex, trainloader, testloader, optimizer, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:0] Test Acc: 9.520% | Loss: 2.304% | Train Ops: 0 | Test Ops: 3424596912 | Time: 1.183321s | Train Ops/Sec : 0 \n",
      "[Epoch:1] Test Acc: 44.430% | Loss: 1.540% | Train Ops: 2167466400000 | Test Ops: 3424596912 | Time: 1.373083s | Train Ops/Sec : 476998659640 \n",
      "[Epoch:2] Test Acc: 53.390% | Loss: 1.283% | Train Ops: 4334932800000 | Test Ops: 3424596912 | Time: 1.137444s | Train Ops/Sec : 565688446328 \n",
      "[Epoch:3] Test Acc: 60.660% | Loss: 1.116% | Train Ops: 6502399200000 | Test Ops: 3424596912 | Time: 1.191456s | Train Ops/Sec : 599838397616 \n",
      "[Epoch:4] Test Acc: 63.650% | Loss: 1.020% | Train Ops: 8669865600000 | Test Ops: 3424596912 | Time: 1.163049s | Train Ops/Sec : 619864624199 \n",
      "[Epoch:5] Test Acc: 64.870% | Loss: 0.983% | Train Ops: 10837332000000 | Test Ops: 3424596912 | Time: 1.589778s | Train Ops/Sec : 617010510324 \n",
      "[Epoch:6] Test Acc: 69.120% | Loss: 0.872% | Train Ops: 13004798400000 | Test Ops: 3424596912 | Time: 1.135618s | Train Ops/Sec : 628838530556 \n",
      "[Epoch:7] Test Acc: 71.390% | Loss: 0.814% | Train Ops: 15172264800000 | Test Ops: 3424596912 | Time: 1.160570s | Train Ops/Sec : 636888840139 \n",
      "[Epoch:8] Test Acc: 72.210% | Loss: 0.789% | Train Ops: 17339731200000 | Test Ops: 3424596912 | Time: 1.161587s | Train Ops/Sec : 642771817560 \n",
      "[Epoch:9] Test Acc: 72.790% | Loss: 0.782% | Train Ops: 19507197600000 | Test Ops: 3424596912 | Time: 1.168267s | Train Ops/Sec : 647559787006 \n",
      "Time elapsed: 30.124s | Total Ops: 97570233969120 | Total Parameters : 1069822 \n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "netVGG = VGG().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(netVGG.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "epoch_nums = 10\n",
    "\n",
    "train_model(epoch_nums, netVGG, trainloader, testloader, optimizer, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:0] Test Acc: 8.830% | Loss: 2.307% | Train Ops: 0 | Test Ops: 6253260800 | Time: 1.790318s | Train Ops/Sec : 0 \n",
      "[Epoch:1] Test Acc: 56.440% | Loss: 1.224% | Train Ops: 3957760000000 | Test Ops: 6253260800 | Time: 1.767439s | Train Ops/Sec : 276940073380 \n",
      "[Epoch:2] Test Acc: 62.830% | Loss: 1.045% | Train Ops: 7915520000000 | Test Ops: 6253260800 | Time: 1.782650s | Train Ops/Sec : 295267498390 \n",
      "[Epoch:3] Test Acc: 69.030% | Loss: 0.889% | Train Ops: 11873280000000 | Test Ops: 6253260800 | Time: 1.776960s | Train Ops/Sec : 301976290765 \n",
      "[Epoch:4] Test Acc: 65.380% | Loss: 1.042% | Train Ops: 15831040000000 | Test Ops: 6253260800 | Time: 1.790285s | Train Ops/Sec : 305354740214 \n",
      "[Epoch:5] Test Acc: 69.640% | Loss: 0.872% | Train Ops: 19788800000000 | Test Ops: 6253260800 | Time: 1.764334s | Train Ops/Sec : 307538372201 \n",
      "[Epoch:6] Test Acc: 75.050% | Loss: 0.722% | Train Ops: 23746560000000 | Test Ops: 6253260800 | Time: 1.786782s | Train Ops/Sec : 308922587753 \n",
      "[Epoch:7] Test Acc: 73.730% | Loss: 0.781% | Train Ops: 27704320000000 | Test Ops: 6253260800 | Time: 1.792769s | Train Ops/Sec : 309906464217 \n",
      "[Epoch:8] Test Acc: 74.160% | Loss: 0.773% | Train Ops: 31662080000000 | Test Ops: 6253260800 | Time: 1.773321s | Train Ops/Sec : 310710793435 \n",
      "[Epoch:9] Test Acc: 76.270% | Loss: 0.694% | Train Ops: 35619840000000 | Test Ops: 6253260800 | Time: 1.797850s | Train Ops/Sec : 311267826692 \n",
      "Time elapsed: 114.435s | Total Ops: 178161732608000 | Total Parameters : 160074 \n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "netRes = ResNet18().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(netRes.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "epoch_nums = 10\n",
    "\n",
    "train_model(epoch_nums, netRes, trainloader, testloader, optimizer, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
