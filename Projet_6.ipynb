{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train) \n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2) \n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instrumentation et évaluation \"en continu\" du système #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifier les fonctions pour calculer à chaque étape le nombre d'opérations flottantes effectuées, séparément pour les additions, les multiplications, les maximums et le total. ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def count_maxpool_operations(self, input, output, pool_layer, out_channels):\n",
    "        kernel_maxpooling = pool_layer.kernel_size\n",
    "        stride = pool_layer.stride\n",
    "        padding = pool_layer.padding\n",
    "        output_height = output.shape[2]\n",
    "        output_width = output.shape[3]\n",
    "        out_channels =  output.shape[1]\n",
    "        num_max = output_height * output_width * (kernel_maxpooling**2 -1) * out_channels\n",
    "        return num_max\n",
    "\n",
    "    def count_conv_operations(self, input, output, output_pooled, conv_layer, pool_layer):\n",
    "        out_channels, in_channels = output.size(1), conv_layer.in_channels\n",
    "        output_height, output_width = output.size(2), output.size(3)\n",
    "        filter_size = conv_layer.kernel_size[0]\n",
    "        stride = conv_layer.stride[0]\n",
    "        padding = conv_layer.padding[0]\n",
    "        num_mults = output_height * output_width * in_channels * filter_size ** 2 * out_channels\n",
    "        num_adds = output_height * output_width * in_channels * filter_size ** 2 * out_channels\n",
    "        num_maxs = self.count_maxpool_operations(output, output_pooled, pool_layer, out_channels)\n",
    "        total_ops = num_mults + num_adds + num_maxs\n",
    "        return num_mults, num_adds, num_maxs, total_ops\n",
    "\n",
    "    def count_operations(self, x):\n",
    "        conv1_out = self.conv1(x)\n",
    "        conv1_out_pooled = self.pool(F.relu(conv1_out))\n",
    "        conv2_out = self.conv2(conv1_out_pooled)\n",
    "        conv2_out_pooled = self.pool(F.relu(conv2_out))\n",
    "        conv1_ops = self.count_conv_operations(x, conv1_out, conv1_out_pooled, self.conv1, self.pool)\n",
    "        conv2_ops = self.count_conv_operations(conv1_out_pooled, conv2_out, conv2_out_pooled, self.conv2, self.pool)\n",
    "        return conv1_ops, conv2_ops        \n",
    "\n",
    "    def count_fc_operations(self, input, fc_layer):\n",
    "        in_features = fc_layer.in_features\n",
    "        out_features = fc_layer.out_features\n",
    "        num_mults = out_features * in_features\n",
    "        num_adds = out_features * in_features\n",
    "        num_maxs = 0    \n",
    "        total_ops = num_mults + num_adds\n",
    "        return num_mults, num_adds, num_maxs, total_ops\n",
    "\n",
    "    def count_total_operations(self, x):\n",
    "        conv1_ops, conv2_ops = self.count_operations(x)\n",
    "        fc1_ops = self.count_fc_operations(x, self.fc1)\n",
    "        fc2_ops = self.count_fc_operations(x, self.fc2)\n",
    "        fc3_ops = self.count_fc_operations(x, self.fc3)\n",
    "        total_ops = sum(op[3] for op in [conv1_ops, conv2_ops, fc1_ops, fc2_ops, fc3_ops])\n",
    "        return total_ops\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dans la partie entraînement du réseau CNN, lister les différentes couches et sous-couches. ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La couche de convolution : ###\n",
    "La couche de convolution applique un ensemble de filtres convolutifs aux images en entrée, chacun d'entre eux activant certaines caractéristiques des images. \n",
    "\n",
    "- `self.conv1` Une première couche de convolution prend en entrée des images RGB et produit 6 filtres de convolution de taille 5×5 sans padding.\n",
    "- `self.conv2` Une deuxième couche de convolution prend 6 entrées et produit 16 filtres de convolution de taille 5×5 sans padding.\n",
    "\n",
    "### Les couches entièrement connectées : ###\n",
    "Chaque couche entièrement connectée effectue une transformation linéaire suivie d'une activation ReLU, qui introduit de la non-linéarité dans le réseau.  Du coup, dans chacune des couches entièrement connectées (`self.fc1`, `self.fc2` et `self.fc3`), il y a une sous-couche linéaire suivie d'une sous-couche non linéaire. Ces couches sont responsables de la combinaison des caractéristiques extraites par les couches de convolution précédentes pour effectuer la tâche de classification finale.\n",
    "\n",
    "`self.fc1`: \n",
    "- Il y a 120 neurones dans la couche entièrement connectée, chacun connecté à une entrée de taille 400.\n",
    "- Sous-couche linéaire : Elle effectue une transformation linéaire des caractéristiques d'entrée.\n",
    "- Sous-couche non linéaire : Suite à la transformation linéaire, une activation ReLU est appliquée. Cela introduit de la non-linéarité dans la sortie de la couche.\n",
    "\n",
    "`self.fc2`:\n",
    "- 84 neurones dans la deuxième couche entièrement connectée, chacun connecté aux 120 neurones de la couche précédente.\n",
    "- Sous-couche linéaire : Elle effectue une transformation linéaire des caractéristiques d'entrée.\n",
    "- Sous-couche non linéaire : Suite à la transformation linéaire, une activation ReLU est appliquée. Cela introduit de la non-linéarité dans la sortie de la couche.\n",
    "\n",
    "`self.fc3`:\n",
    "- 10 neurones dans la dernière couche entièrement connectée, chacun connecté aux 84 neurones de la couche précédente.\n",
    "- Sous-couche linéaire : Il s'agit de la dernière transformation linéaire qui produit la sortie finale du réseau sans RELU.\n",
    "\n",
    "### La couche de pooling : ###\n",
    "L'opération de pooling consiste à réduire la taille des images, tout en préservant leurs caractéristiques importantes. Elle est utilisée après chaque couche de convolution.\n",
    "- `self.pool` Cette couche en effectuant une opération de max pooling avec une fenêtre de taille 2x2 et un pas de 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Donner la taille des différents tenseurs de données Xn et de poids Wn le long du calcul. ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Layer 1 (conv1): ###\n",
    "- X1 = 3×32×32 = 3072\n",
    "- Poids W1:\n",
    "    - [6, 3, 5, 5]: Poids de convolution: 6×3×5×5=450\n",
    "    - Bias: [6]\n",
    "\n",
    "### Convolutional Layer 2 (conv2): ###\n",
    "- X2 = 6×14×14 = 1176\n",
    "- Poids W2:\n",
    "    - [16, 6, 5, 5]: Poids de convolution: 16×6×5×5=2400\n",
    "    - Bias: [16]\n",
    "\n",
    "### Fully Connected Layer 1 (fc1): ###\n",
    "- X3 = 16×5×5 = 400\n",
    "- Poids W3:\n",
    "    - [120, 400]: Poids de convolution: 120×400 = 48000\n",
    "    - Bias: [120]\n",
    "\n",
    "### Fully Connected Layer 2 (fc2): ###\n",
    "- X4 = 120\n",
    "- Poids W4:\n",
    "    - [84, 120]: Poids de convolution: 84×120=10080\n",
    "    - Bias: [84]\n",
    "  \n",
    "### Fully Connected Layer 3 (fc3): ###\n",
    "- X5 = 84\n",
    "- Poids W5:\n",
    "    - [10, 84]: Poids de convolution: 10×84=840\n",
    "    - Bias: [10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction spécifique pour le calcul de la précision globale, de l'erreur, du nombre d'opérations flottantes effectuées et du nombre total d'opérations flottantes par secondesur l'ensemble de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, inchannel, outchannel, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inchannel, outchannel, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(outchannel)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(outchannel, outchannel, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(outchannel)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or inchannel != outchannel:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(inchannel, outchannel, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(outchannel)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self.make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.fc = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def make_layer(self, block, channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        inchannel = 64 \n",
    "        for stride in strides:\n",
    "            layers.append(block(inchannel, channels, stride))\n",
    "            inchannel = channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer1(out)\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "    def count_conv_operations(self, input, output, output_pooled, stride, padding, kernel_size):\n",
    "        out_channels = output.size(1)\n",
    "        output_height, output_width = output.size(2), output.size(3)\n",
    "        num_mults = output_height * output_width * input.size(1) * kernel_size ** 2 * out_channels\n",
    "        num_adds = output_height * output_width * input.size(1) * kernel_size ** 2 * out_channels\n",
    "        num_maxs = self.count_maxpool_operations(output, output_pooled)\n",
    "        total_ops = num_mults + num_adds + num_maxs\n",
    "        return num_mults, num_adds, num_maxs, total_ops\n",
    "\n",
    "    def count_maxpool_operations(self, output, output_pooled):\n",
    "        kernel_size = 2 \n",
    "        stride = 2 \n",
    "        padding = 0\n",
    "        output_height = output.shape[2]\n",
    "        output_width = output.shape[3]\n",
    "        out_channels = output.shape[1]\n",
    "        num_max = ((output_height + 2 * padding - kernel_size) // stride + 1) * ((output_width + 2 * padding - kernel_size) // stride + 1) * (kernel_size ** 2 - 1) * out_channels\n",
    "        return num_max\n",
    "\n",
    "\n",
    "    def count_fc_operations(self, input, fc_layer):\n",
    "        in_features = fc_layer.in_features\n",
    "        out_features = fc_layer.out_features    \n",
    "        num_mults = out_features * in_features\n",
    "        num_adds = out_features * in_features\n",
    "        num_maxs = 0\n",
    "        total_ops = num_mults + num_adds\n",
    "        return num_mults, num_adds, num_maxs, total_ops\n",
    "\n",
    "    def count_total_operations(self, x):\n",
    "        conv1_out = self.conv1(x)\n",
    "        conv1_out_pooled = self.relu(self.bn1(conv1_out))\n",
    "        conv1_ops = self.count_conv_operations(x, conv1_out, conv1_out_pooled, self.conv1, None, kernel_size=3)\n",
    "\n",
    "        layer1_out = self.layer1(conv1_out_pooled)\n",
    "        layer1_out_pooled = F.avg_pool2d(layer1_out, 8)\n",
    "        layer1_ops = self.count_conv_operations(conv1_out_pooled, layer1_out, layer1_out_pooled, stride=1, padding=1, kernel_size=3)\n",
    "\n",
    "        fc_ops = self.count_fc_operations(x, self.fc)\n",
    "\n",
    "        total_ops = sum(op[3] for op in [conv1_ops, layer1_ops, fc_ops])\n",
    "        return total_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet18():\n",
    "    return ResNet(ResidualBlock, [2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "  \n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, stride=1, padding=2)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=48, kernel_size=3, padding=1)\n",
    "        self.batchnorm = nn.BatchNorm2d(48)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n",
    "        self.conv3 = nn.Conv2d(in_channels=48, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=64, out_channels=48, kernel_size=3, padding=1)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=3*3*48, out_features=128)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.maxpool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = self.maxpool3(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def count_maxpool_operations(self, input, output, pool_layer):\n",
    "        if pool_layer:\n",
    "            kernel_size = pool_layer.kernel_size\n",
    "            stride = pool_layer.stride\n",
    "            padding = pool_layer.padding\n",
    "            output_height = output.shape[2]\n",
    "            output_width = output.shape[3]\n",
    "            out_channels = output.shape[1]\n",
    "            num_max = ((output_height + 2 * padding - kernel_size) // stride + 1) * ((output_width + 2 * padding - kernel_size) // stride + 1) * (kernel_size ** 2 - 1) * out_channels  \n",
    "        else:\n",
    "            num_max = 0\n",
    "        return num_max\n",
    "\n",
    "    def count_conv_operations(self, input, output, output_pooled, conv_layer, pool_layer):\n",
    "        out_channels, in_channels = output.size(1), conv_layer.in_channels\n",
    "        output_height, output_width = output.size(2), output.size(3)\n",
    "        filter_size = conv_layer.kernel_size[0]\n",
    "        stride = conv_layer.stride[0]\n",
    "        padding = conv_layer.padding[0]\n",
    "        num_mults = output_height * output_width * in_channels * filter_size ** 2 * out_channels\n",
    "        num_adds = output_height * output_width * in_channels * filter_size ** 2 * out_channels\n",
    "        num_maxs = self.count_maxpool_operations(output, output_pooled, pool_layer)\n",
    "        if pool_layer:\n",
    "            num_maxs = self.count_maxpool_operations(output, output_pooled, pool_layer)\n",
    "        else:\n",
    "            num_maxs = 0\n",
    "        total_ops = num_mults + num_adds + num_maxs\n",
    "        return num_mults, num_adds, num_maxs, total_ops\n",
    "\n",
    "    def count_operations(self, x):\n",
    "        conv1_out = F.relu(self.conv1(x))\n",
    "        conv1_out_pooled = self.maxpool1(conv1_out)\n",
    "        conv2_out = F.relu(self.conv2(conv1_out_pooled))\n",
    "        conv2_out_norm = self.batchnorm(conv2_out)\n",
    "        conv2_out_pooled = self.maxpool2(conv2_out_norm)\n",
    "        conv3_out = F.relu(self.conv3(conv2_out_pooled))\n",
    "        conv4_out = F.relu(self.conv4(conv3_out))\n",
    "        conv5_out = F.relu(self.conv5(conv4_out))\n",
    "        conv5_out_pooled = self.maxpool3(conv5_out)\n",
    "        \n",
    "        conv1_ops = self.count_conv_operations(x, conv1_out, conv1_out_pooled, self.conv1, self.maxpool1)\n",
    "        conv2_ops = self.count_conv_operations(conv1_out_pooled, conv2_out, conv2_out_pooled, self.conv2, self.maxpool2)\n",
    "        conv3_ops = self.count_conv_operations(conv2_out_pooled, conv3_out, conv3_out, self.conv3, None)\n",
    "        conv4_ops = self.count_conv_operations(conv3_out, conv4_out, conv4_out, self.conv4, None)\n",
    "        conv5_ops = self.count_conv_operations(conv4_out, conv5_out, conv5_out_pooled, self.conv5, self.maxpool3)\n",
    "        \n",
    "        return conv1_ops, conv2_ops, conv3_ops, conv4_ops, conv5_ops\n",
    "        \n",
    "    def count_fc_operations(self, input, fc_layer):\n",
    "        in_features = fc_layer.in_features\n",
    "        out_features = fc_layer.out_features\n",
    "        num_mults = out_features * in_features\n",
    "        num_adds = out_features * in_features\n",
    "        num_maxs = 0\n",
    "        total_ops = num_mults + num_adds\n",
    "        return num_mults, num_adds, num_maxs, total_ops\n",
    "    \n",
    "    def count_total_operations(self, x):\n",
    "        conv1_ops, conv2_ops, conv3_ops, conv4_ops, conv5_ops = self.count_operations(x)\n",
    "        fc1_ops = self.count_fc_operations(x, self.fc1)\n",
    "        fc2_ops = self.count_fc_operations(x, self.fc2)\n",
    "        fc3_ops = self.count_fc_operations(x, self.fc3)\n",
    "        total_ops = sum(op[3] for op in [conv1_ops, conv2_ops, conv3_ops, conv4_ops, conv5_ops, fc1_ops, fc2_ops, fc3_ops])\n",
    "        return total_ops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def count_maxpool_operations(self, input, output, pool_layer, out_channels):\n",
    "        kernel_maxpooling = pool_layer.kernel_size\n",
    "        stride = pool_layer.stride\n",
    "        padding = pool_layer.padding\n",
    "        output_height = output.shape[2]\n",
    "        output_width = output.shape[3]\n",
    "        out_channels =  output.shape[1]\n",
    "        num_max = output_height * output_width * (kernel_maxpooling**2 -1) * out_channels\n",
    "        return num_max\n",
    "\n",
    "    def count_conv_operations(self, input, output, output_pooled, conv_layer, pool_layer):\n",
    "        out_channels, in_channels = output.size(1), conv_layer.in_channels\n",
    "        output_height, output_width = output.size(2), output.size(3)\n",
    "        filter_size = conv_layer.kernel_size[0]\n",
    "        stride = conv_layer.stride[0]\n",
    "        padding = conv_layer.padding[0]\n",
    "        num_mults = output_height * output_width * in_channels * filter_size ** 2 * out_channels\n",
    "        num_adds = output_height * output_width * in_channels * filter_size ** 2 * out_channels\n",
    "        num_maxs = self.count_maxpool_operations(output, output_pooled, pool_layer, out_channels)\n",
    "        total_ops = num_mults + num_adds + num_maxs\n",
    "        return num_mults, num_adds, num_maxs, total_ops\n",
    "\n",
    "    def count_operations(self, x):\n",
    "        conv1_out = self.conv1(x)\n",
    "        conv1_out_pooled = self.pool(F.relu(conv1_out))\n",
    "        conv2_out = self.conv2(conv1_out_pooled)\n",
    "        conv2_out_pooled = self.pool(F.relu(conv2_out))\n",
    "        conv1_ops = self.count_conv_operations(x, conv1_out, conv1_out_pooled, self.conv1, self.pool)\n",
    "        conv2_ops = self.count_conv_operations(conv1_out_pooled, conv2_out, conv2_out_pooled, self.conv2, self.pool)\n",
    "        return conv1_ops, conv2_ops        \n",
    "\n",
    "    def count_fc_operations(self, input, fc_layer):\n",
    "        in_features = fc_layer.in_features\n",
    "        out_features = fc_layer.out_features\n",
    "        num_mults = out_features * in_features\n",
    "        num_adds = out_features * in_features\n",
    "        num_maxs = 0    \n",
    "        total_ops = num_mults + num_adds\n",
    "        return num_mults, num_adds, num_maxs, total_ops\n",
    "\n",
    "    def count_total_operations(self, x):\n",
    "        conv1_ops, conv2_ops = self.count_operations(x)\n",
    "        fc1_ops = self.count_fc_operations(x, self.fc1)\n",
    "        fc2_ops = self.count_fc_operations(x, self.fc2)\n",
    "        fc3_ops = self.count_fc_operations(x, self.fc3)\n",
    "        total_ops = sum(op[3] for op in [conv1_ops, conv2_ops, fc1_ops, fc2_ops, fc3_ops])\n",
    "        return total_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(net, testloader, criterion, device):\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total_samples = 0 \n",
    "    total_loss = 0.0\n",
    "    total_test_ops = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total_test_ops += net.count_total_operations(images)\n",
    "\n",
    "        evaluate_time = time.time() - start_time\n",
    "\n",
    "    test_accuracy = 100. * correct / total_samples\n",
    "    total_loss /= len(testloader)\n",
    "    ops_per_second = total_test_ops / evaluate_time\n",
    "\n",
    "    return test_accuracy, total_loss, total_test_ops, evaluate_time, ops_per_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epoch_nums, net, trainloader, testloader, optimizer, criterion, device):\n",
    "    total_time = 0.0\n",
    "    total_ops = 0\n",
    "    epoch_train_ops = 0\n",
    "    \n",
    "    for epoch in range(epoch_nums):\n",
    "        net.train()\n",
    "        sum_loss = 0.0\n",
    "        correct = 0.0\n",
    "        total = 0.0\n",
    "        iteration_num = 0\n",
    "        total_train_ops = 0\n",
    "\n",
    "        if epoch > 0: \n",
    "            for i, data in enumerate(trainloader, 0):\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "    \n",
    "                if epoch == 0 and i == 0: \n",
    "                    batch_train_ops = net.count_total_operations(inputs)\n",
    "                    batch_train_ops = inputs.shape[0] * batch_train_ops\n",
    "                    epoch_train_ops += batch_train_ops\n",
    "    \n",
    "                if torch.cuda.is_available(): \n",
    "                    torch.cuda.synchronize()\n",
    "                batch_train_start = time.time()\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                sum_loss += loss.item()\n",
    "    \n",
    "                if torch.cuda.is_available(): \n",
    "                    torch.cuda.synchronize()\n",
    "                batch_train_time = time.time() - batch_train_start\n",
    "    \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                iteration_num += 1\n",
    "    \n",
    "                batch_train_ops = net.count_total_operations(inputs)\n",
    "                batch_train_ops = inputs.shape[0] * batch_train_ops\n",
    "                total_train_ops += batch_train_ops\n",
    "    \n",
    "            epoch_train_ops += total_train_ops\n",
    "\n",
    "        test_accuracy, total_loss, total_test_ops, evaluate_time, ops_per_second = test_model(net, testloader, criterion, device)\n",
    "        total_time += evaluate_time\n",
    "\n",
    "        print('[Epoch:%d] Test Acc: %.3f%% | Loss: %.3f%% | Train Ops: %d | Test Ops: %d | Time: %.6fs | Ops/Sec : %d ' % (\n",
    "            epoch, \n",
    "            test_accuracy,\n",
    "            total_loss,\n",
    "            epoch_train_ops,\n",
    "            total_test_ops,\n",
    "            evaluate_time,\n",
    "            ops_per_second\n",
    "        ))\n",
    "\n",
    "        if epoch > 0: \n",
    "            total_ops += total_test_ops + epoch_train_ops\n",
    "            ops_per_second = total_ops / total_time \n",
    "            total_params = sum(p.numel() for p in net.parameters())\n",
    "    \n",
    "    print('Time elapsed: %.3fs | Total ops: %d | Ops/Second : %d | Total Parameters : %d ' % (\n",
    "        total_time,\n",
    "        total_ops,\n",
    "        ops_per_second,\n",
    "        total_params,\n",
    "    ))\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:0] Test Acc: 10.000% | Loss: 2.304% | Train Ops: 0 | Test Ops: 103345272 | Time: 1.192837s | Ops/Sec : 86638200 \n",
      "[Epoch:1] Test Acc: 15.090% | Loss: 2.296% | Train Ops: 65408400000 | Test Ops: 103345272 | Time: 1.197039s | Ops/Sec : 86334080 \n",
      "[Epoch:2] Test Acc: 19.270% | Loss: 2.264% | Train Ops: 130816800000 | Test Ops: 103345272 | Time: 1.153009s | Ops/Sec : 89630955 \n",
      "[Epoch:3] Test Acc: 25.530% | Loss: 2.075% | Train Ops: 196225200000 | Test Ops: 103345272 | Time: 1.177865s | Ops/Sec : 87739485 \n",
      "[Epoch:4] Test Acc: 29.210% | Loss: 1.943% | Train Ops: 261633600000 | Test Ops: 103345272 | Time: 1.342461s | Ops/Sec : 76981934 \n",
      "Time elapsed: 6.063s | Total ops: 654497381088 | Ops/Second : 107945663358 | Total Parameters : 62006 \n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "net = Net().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)    \n",
    "epoch_nums = 5\n",
    "\n",
    "train_model(epoch_nums, net, trainloader, testloader, optimizer, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:0] Test Acc: 10.000% | Loss: 2.304% | Train Ops: 0 | Test Ops: 1165701248 | Time: 1.179255s | Ops/Sec : 988506705 \n",
      "[Epoch:1] Test Acc: 35.110% | Loss: 1.699% | Train Ops: 737785600000 | Test Ops: 1165701248 | Time: 1.158652s | Ops/Sec : 1006084156 \n",
      "[Epoch:2] Test Acc: 45.490% | Loss: 1.438% | Train Ops: 1475571200000 | Test Ops: 1165701248 | Time: 1.177061s | Ops/Sec : 990348785 \n",
      "[Epoch:3] Test Acc: 53.950% | Loss: 1.242% | Train Ops: 2213356800000 | Test Ops: 1165701248 | Time: 1.461486s | Ops/Sec : 797613639 \n",
      "[Epoch:4] Test Acc: 59.670% | Loss: 1.100% | Train Ops: 2951142400000 | Test Ops: 1165701248 | Time: 1.184581s | Ops/Sec : 984062050 \n",
      "Time elapsed: 6.161s | Total ops: 7382518804992 | Ops/Second : 1198259502184 | Total Parameters : 173834 \n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "netAlex = AlexNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(netAlex.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "epoch_nums = 5\n",
    "\n",
    "train_model(epoch_nums, netAlex, trainloader, testloader, optimizer, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:0] Test Acc: 10.000% | Loss: 2.306% | Train Ops: 0 | Test Ops: 3424596912 | Time: 1.165863s | Ops/Sec : 2937392732 \n",
      "[Epoch:1] Test Acc: 43.780% | Loss: 1.522% | Train Ops: 2167466400000 | Test Ops: 3424596912 | Time: 1.336195s | Ops/Sec : 2562946059 \n",
      "[Epoch:2] Test Acc: 50.390% | Loss: 1.370% | Train Ops: 4334932800000 | Test Ops: 3424596912 | Time: 1.141114s | Ops/Sec : 3001100988 \n",
      "[Epoch:3] Test Acc: 57.300% | Loss: 1.188% | Train Ops: 6502399200000 | Test Ops: 3424596912 | Time: 1.177423s | Ops/Sec : 2908552755 \n",
      "[Epoch:4] Test Acc: 62.430% | Loss: 1.050% | Train Ops: 8669865600000 | Test Ops: 3424596912 | Time: 1.166243s | Ops/Sec : 2936436135 \n",
      "Time elapsed: 5.987s | Total ops: 21688362387648 | Ops/Second : 3622674374693 | Total Parameters : 1069822 \n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "netVGG = VGG().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(netVGG.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "epoch_nums = 5\n",
    "\n",
    "train_model(epoch_nums, netVGG, trainloader, testloader, optimizer, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:0] Test Acc: 9.470% | Loss: 2.310% | Train Ops: 0 | Test Ops: 6253260800 | Time: 1.190824s | Ops/Sec : 5251206842 \n",
      "[Epoch:1] Test Acc: 58.310% | Loss: 1.184% | Train Ops: 3957760000000 | Test Ops: 6253260800 | Time: 1.222573s | Ops/Sec : 5114837154 \n",
      "[Epoch:2] Test Acc: 64.520% | Loss: 1.015% | Train Ops: 7915520000000 | Test Ops: 6253260800 | Time: 1.181114s | Ops/Sec : 5294376565 \n",
      "[Epoch:3] Test Acc: 66.100% | Loss: 0.974% | Train Ops: 11873280000000 | Test Ops: 6253260800 | Time: 1.475332s | Ops/Sec : 4238546123 \n",
      "[Epoch:4] Test Acc: 70.990% | Loss: 0.837% | Train Ops: 15831040000000 | Test Ops: 6253260800 | Time: 1.192382s | Ops/Sec : 5244345176 \n",
      "Time elapsed: 6.262s | Total ops: 39602613043200 | Ops/Second : 6324050022163 | Total Parameters : 160074 \n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "netRes = ResNet18().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(netRes.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "epoch_nums = 5\n",
    "\n",
    "train_model(epoch_nums, netRes, trainloader, testloader, optimizer, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}